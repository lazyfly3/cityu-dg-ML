# Assignment 1 å­¦ä¹ ç¬”è®° / Study Notes

## ç›®å½• / Table of Contents
1. [æ¦‚ç‡è®ºåŸºç¡€ / Probability Fundamentals](#æ¦‚ç‡è®ºåŸºç¡€)
2. [æœ´ç´ è´å¶æ–¯ / Naive Bayes](#æœ´ç´ è´å¶æ–¯)
3. [çº¿æ€§åˆ¤åˆ«åˆ†æ / Linear Discriminant Analysis](#çº¿æ€§åˆ¤åˆ«åˆ†æ)
4. [KLæ•£åº¦ / KL Divergence](#klæ•£åº¦)

---

## æ¦‚ç‡è®ºåŸºç¡€ / Probability Fundamentals

### 1.1 æœŸæœ›å€¼ / Expected Value

**ä¸­æ–‡è§£é‡Šï¼š**
æœŸæœ›å€¼æ˜¯éšæœºå˜é‡æ‰€æœ‰å¯èƒ½å–å€¼çš„åŠ æƒå¹³å‡ï¼Œæƒé‡æ˜¯æ¯ä¸ªå€¼å‡ºç°çš„æ¦‚ç‡ã€‚

**English Explanation:**
The expected value is the weighted average of all possible values of a random variable, where the weights are the probabilities of each value.

**æ•°å­¦å®šä¹‰ / Mathematical Definition:**  
$$
E[X] = \sum_{x\in\mathcal{X}} x \cdot P(X = x)
$$

**ç¬¦å·è¯´æ˜ / Symbol Explanation:**
- $E[X]$ï¼šéšæœºå˜é‡Xçš„æœŸæœ›å€¼ / Expected value of random variable X
- $\sum_{x\in\mathcal{X}}$ï¼šå¯¹æ‰€æœ‰å¯èƒ½çš„xå€¼æ±‚å’Œ / Sum over all possible values of x
- $x$ï¼šéšæœºå˜é‡çš„æŸä¸ªå–å€¼ / A specific value of the random variable
- $P(X = x)$ï¼šXå–å€¼ä¸ºxçš„æ¦‚ç‡ / Probability that X equals x
- $\mathcal{X}$ï¼šXæ‰€æœ‰å¯èƒ½å–å€¼çš„é›†åˆ / Set of all possible values of X

**è®¡ç®—æ­¥éª¤ / Calculation Steps:**
1. åˆ—å‡ºXæ‰€æœ‰å¯èƒ½çš„å–å€¼ / List all possible values of X
2. å¯¹æ¯ä¸ªå–å€¼xï¼Œè®¡ç®— x Ã— P(X=x) / For each value x, calculate x Ã— P(X=x)
3. å°†æ‰€æœ‰ç»“æœç›¸åŠ  / Sum all the results

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
å‡è®¾ä¸€ä¸ªä¸å…¬å¹³çš„éª°å­ï¼Œå„é¢æ¦‚ç‡ä¸ºï¼šP(1)=0.1, P(2)=0.1, P(3)=0.2, P(4)=0.2, P(5)=0.4, P(6)=0
Suppose an unfair die with probabilities: P(1)=0.1, P(2)=0.1, P(3)=0.2, P(4)=0.2, P(5)=0.4, P(6)=0

è®¡ç®—æœŸæœ›å€¼ / Calculate expected value:
- E[X] = 1Ã—0.1 + 2Ã—0.1 + 3Ã—0.2 + 4Ã—0.2 + 5Ã—0.4 + 6Ã—0
- E[X] = 0.1 + 0.2 + 0.6 + 0.8 + 2.0 + 0
- E[X] = 3.7

**ç¤ºä¾‹ / Example:**
- å…¬å¹³éª°å­ï¼šE[X] = 1Ã—1/6 + 2Ã—1/6 + 3Ã—1/6 + 4Ã—1/6 + 5Ã—1/6 + 6Ã—1/6 = 21/6 = 3.5
- Fair die: E[X] = 1Ã—1/6 + 2Ã—1/6 + 3Ã—1/6 + 4Ã—1/6 + 5Ã—1/6 + 6Ã—1/6 = 21/6 = 3.5

### 1.2 æŒ‡ç¤ºå‡½æ•° / Indicator Function

**ä¸­æ–‡è§£é‡Šï¼š**
æŒ‡ç¤ºå‡½æ•° I[X = a] æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å‡½æ•°ï¼Œå½“äº‹ä»¶å‘ç”Ÿæ—¶å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚

**English Explanation:**
The indicator function I[X = a] is a special function that equals 1 when the event occurs, and 0 otherwise.

**å®šä¹‰ / Definition:**  
$$
I[X=a] = 
\begin{cases}
1, & X=a \\
0, & \text{otherwise}
\end{cases}
$$

**é‡è¦æ€§è´¨ / Important Property:**  
$$
E[I[X=a]] = P(X=a)
$$

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
å‡è®¾Xå¯ä»¥å–{3, 8, 9}ï¼Œæ¦‚ç‡åˆ†åˆ«ä¸ºP(3)=0.3, P(8)=0.5, P(9)=0.2
Suppose X can take {3, 8, 9} with probabilities P(3)=0.3, P(8)=0.5, P(9)=0.2

è®¡ç®—E[I[X=8]] / Calculate E[I[X=8]]:
- å½“X=8æ—¶ï¼ŒI[X=8]=1ï¼Œæ¦‚ç‡ä¸º0.5 / When X=8, I[X=8]=1 with probability 0.5
- å½“Xâ‰ 8æ—¶ï¼ˆX=3æˆ–9ï¼‰ï¼ŒI[X=8]=0ï¼Œæ¦‚ç‡ä¸º0.3+0.2=0.5 / When Xâ‰ 8 (X=3 or 9), I[X=8]=0 with probability 0.3+0.2=0.5
- E[I[X=8]] = 1Ã—0.5 + 0Ã—0.5 = 0.5 = P(X=8) âœ“

**è¯æ˜ / Proof:**  
$$
E[I[X=a]] = 1\cdot P(X=a) + 0\cdot P(X\neq a) = P(X=a)
$$

### 1.3 ç†µ / Entropy

**ä¸­æ–‡è§£é‡Šï¼š**
ç†µè¡¡é‡éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§æˆ–ä¿¡æ¯é‡ã€‚ç†µè¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ã€‚

**English Explanation:**
Entropy measures the uncertainty or information content of a random variable. Higher entropy means greater uncertainty.

**å®šä¹‰ / Definition:**  
$$
H(X) = -\sum_{x\in\mathcal{X}} P(X=x)\log_2 P(X=x) = -E[\log_2 P(X)]
$$

**ç¬¦å·è¯´æ˜ / Symbol Explanation:**
- $H(X)$ï¼šéšæœºå˜é‡Xçš„ç†µ / Entropy of random variable X
- $\sum_{x\in\mathcal{X}}$ï¼šå¯¹æ‰€æœ‰å¯èƒ½çš„xå€¼æ±‚å’Œ / Sum over all possible values
- $P(X=x)$ï¼šXå–å€¼ä¸ºxçš„æ¦‚ç‡ / Probability that X equals x
- $\log_2$ï¼šä»¥2ä¸ºåº•çš„å¯¹æ•°ï¼ˆå•ä½æ˜¯æ¯”ç‰¹/bitï¼‰/ Base-2 logarithm (unit is bits)
- è´Ÿå·ï¼šç¡®ä¿ç†µä¸ºéè´Ÿå€¼ / Negative sign ensures non-negative entropy

**è®¡ç®—æ­¥éª¤ / Calculation Steps:**
1. å¯¹æ¯ä¸ªå¯èƒ½çš„xå€¼ï¼Œè®¡ç®— $P(X=x) \times \log_2 P(X=x)$ / For each x, calculate $P(X=x) \times \log_2 P(X=x)$
2. å°†æ‰€æœ‰ç»“æœç›¸åŠ  / Sum all results
3. å–è´Ÿå· / Take negative sign

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
å‡è®¾ä¸€ä¸ªä¸å…¬å¹³ç¡¬å¸ï¼ŒP(æ­£é¢)=0.7, P(åé¢)=0.3
Suppose an unfair coin with P(heads)=0.7, P(tails)=0.3

è®¡ç®—ç†µ / Calculate entropy:
- H(X) = -[P(æ­£é¢)Ã—logâ‚‚(0.7) + P(åé¢)Ã—logâ‚‚(0.3)]
- H(X) = -[0.7Ã—logâ‚‚(0.7) + 0.3Ã—logâ‚‚(0.3)]
- logâ‚‚(0.7) â‰ˆ -0.515, logâ‚‚(0.3) â‰ˆ -1.737
- H(X) = -[0.7Ã—(-0.515) + 0.3Ã—(-1.737)]
- H(X) = -[-0.3605 - 0.5211]
- H(X) = -[-0.8816] = 0.8816 bits

å…¬å¹³ç¡¬å¸ï¼ˆP=0.5ï¼‰çš„ç†µä¸º1 bitï¼Œæ‰€ä»¥ä¸å…¬å¹³ç¡¬å¸çš„ç†µæ›´å°ï¼ˆä¸ç¡®å®šæ€§æ›´å°ï¼‰
Fair coin (P=0.5) has entropy 1 bit, so unfair coin has lower entropy (less uncertainty)

**æ€§è´¨ / Properties:**
- ç†µæ€»æ˜¯éè´Ÿçš„ / Entropy is always non-negative
- å½“æ‰€æœ‰ç»“æœç­‰æ¦‚ç‡æ—¶ï¼Œç†µæœ€å¤§ / Entropy is maximized when all outcomes are equally likely
- å½“åªæœ‰ä¸€ä¸ªç¡®å®šç»“æœæ—¶ï¼Œç†µä¸º0 / Entropy is 0 when there's only one certain outcome

**ç¤ºä¾‹ / Example:**
å…¬å¹³ç¡¬å¸ï¼šH(X) = -0.5Ã—logâ‚‚(0.5) - 0.5Ã—logâ‚‚(0.5) = 1 bit
Fair coin: H(X) = -0.5Ã—logâ‚‚(0.5) - 0.5Ã—logâ‚‚(0.5) = 1 bit

### 1.4 è”åˆç†µä¸æ¡ä»¶ç†µ / Joint and Conditional Entropy

**è”åˆç†µ / Joint Entropy:**  
$$
H(X,Y) = -\sum_{x,y} P(X=x, Y=y)\,\log_2 P(X=x, Y=y)
$$

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
å‡è®¾Xå’ŒYçš„è”åˆåˆ†å¸ƒå¦‚ä¸‹ï¼š
Suppose joint distribution of X and Y:

|     | Y=0 | Y=1 |
|-----|-----|-----|
| X=0 | 0.3 | 0.2 |
| X=1 | 0.1 | 0.4 |

è®¡ç®—è”åˆç†µ / Calculate joint entropy:
- H(X,Y) = -[0.3Ã—logâ‚‚(0.3) + 0.2Ã—logâ‚‚(0.2) + 0.1Ã—logâ‚‚(0.1) + 0.4Ã—logâ‚‚(0.4)]
- H(X,Y) = -[0.3Ã—(-1.737) + 0.2Ã—(-2.322) + 0.1Ã—(-3.322) + 0.4Ã—(-1.322)]
- H(X,Y) = -[-0.521 - 0.464 - 0.332 - 0.529] = 1.846 bits

**æ¡ä»¶ç†µ / Conditional Entropy:**  
$$
H(Y|X) = -\sum_{x,y} P(X=x, Y=y)\,\log_2 P(Y=y\mid X=x)
$$

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
ä½¿ç”¨ä¸Šé¢çš„è”åˆåˆ†å¸ƒï¼Œå…ˆè®¡ç®—æ¡ä»¶æ¦‚ç‡ï¼š
Using above joint distribution, first calculate conditional probabilities:
- P(Y=0|X=0) = 0.3/(0.3+0.2) = 0.6, P(Y=1|X=0) = 0.2/0.5 = 0.4
- P(Y=0|X=1) = 0.1/(0.1+0.4) = 0.2, P(Y=1|X=1) = 0.4/0.5 = 0.8

è®¡ç®—æ¡ä»¶ç†µ / Calculate conditional entropy:
- H(Y|X) = -[0.3Ã—logâ‚‚(0.6) + 0.2Ã—logâ‚‚(0.4) + 0.1Ã—logâ‚‚(0.2) + 0.4Ã—logâ‚‚(0.8)]
- H(Y|X) = -[0.3Ã—(-0.737) + 0.2Ã—(-1.322) + 0.1Ã—(-2.322) + 0.4Ã—(-0.322)]
- H(Y|X) = -[-0.221 - 0.264 - 0.232 - 0.129] = 0.846 bits

**é“¾å¼æ³•åˆ™ / Chain Rule:**  
$$
H(X,Y) = H(Y) + H(X|Y) = H(X) + H(Y|X)
$$

**è¯æ˜æ€è·¯ / Proof Sketch:**
ä½¿ç”¨æ¡ä»¶æ¦‚ç‡çš„å®šä¹‰å’ŒæœŸæœ›çš„çº¿æ€§æ€§è´¨ã€‚
Using the definition of conditional probability and linearity of expectation.

### 1.5 äº’ä¿¡æ¯ / Mutual Information

**ä¸­æ–‡è§£é‡Šï¼š**
äº’ä¿¡æ¯è¡¡é‡ä¸¤ä¸ªéšæœºå˜é‡ä¹‹é—´çš„ç›¸äº’ä¾èµ–ç¨‹åº¦ã€‚

**English Explanation:**
Mutual information measures the mutual dependence between two random variables.

**å®šä¹‰ / Definition:**  
$$
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y)
$$

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
ä½¿ç”¨ä¸Šé¢çš„è”åˆåˆ†å¸ƒï¼Œè®¡ç®—äº’ä¿¡æ¯ï¼š
Using above joint distribution, calculate mutual information:

é¦–å…ˆè®¡ç®—è¾¹é™…åˆ†å¸ƒ / First calculate marginal distributions:
- P(X=0) = 0.3+0.2 = 0.5, P(X=1) = 0.1+0.4 = 0.5
- P(Y=0) = 0.3+0.1 = 0.4, P(Y=1) = 0.2+0.4 = 0.6

è®¡ç®—H(X)å’ŒH(Y) / Calculate H(X) and H(Y):
- H(X) = -[0.5Ã—logâ‚‚(0.5) + 0.5Ã—logâ‚‚(0.5)] = -[-0.5 - 0.5] = 1 bit
- H(Y) = -[0.4Ã—logâ‚‚(0.4) + 0.6Ã—logâ‚‚(0.6)] = -[-0.529 - 0.442] = 0.971 bits

ä»å‰é¢å·²çŸ¥ / From above:
- H(X,Y) = 1.846 bits
- H(Y|X) = 0.846 bits

è®¡ç®—äº’ä¿¡æ¯ / Calculate mutual information:
- I(X;Y) = H(Y) - H(Y|X) = 0.971 - 0.846 = 0.125 bits
- æˆ– / or: I(X;Y) = H(X) + H(Y) - H(X,Y) = 1 + 0.971 - 1.846 = 0.125 bits âœ“

**é‡è¦æ€§è´¨ / Important Property:**
å¦‚æœ X å’Œ Y ç‹¬ç«‹ï¼Œåˆ™ I(X; Y) = 0
If X and Y are independent, then I(X; Y) = 0

**è¯æ˜ / Proof:**
å¦‚æœ X å’Œ Y ç‹¬ç«‹ï¼Œåˆ™ P(X=x, Y=y) = P(X=x)P(Y=y)
If X and Y are independent, then P(X=x, Y=y) = P(X=x)P(Y=y)

å› æ­¤ / Therefore:
```
I(X; Y) = Î£(x,y) P(X=x, Y=y) logâ‚‚ [P(X=x, Y=y) / (P(X=x)P(Y=y))]
        = Î£(x,y) P(X=x, Y=y) logâ‚‚ 1
        = 0
```

---

## æœ´ç´ è´å¶æ–¯ / Naive Bayes

### 2.1 åŸºæœ¬æ€æƒ³ / Basic Idea

**ä¸­æ–‡è§£é‡Šï¼š**
æœ´ç´ è´å¶æ–¯å‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œä½¿ç”¨è´å¶æ–¯å®šç†è¿›è¡Œåˆ†ç±»ã€‚

**English Explanation:**
Naive Bayes assumes features are independent and uses Bayes' theorem for classification.

**è´å¶æ–¯å®šç† / Bayes' Theorem:**  
$$
P(Y\mid X) = \frac{P(X\mid Y)\,P(Y)}{P(X)}
$$

### 2.2 æœ€å¤§ä¼¼ç„¶ä¼°è®¡ / Maximum Likelihood Estimation

**ä¸­æ–‡è§£é‡Šï¼š**
æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯é€‰æ‹©ä½¿è§‚æµ‹æ•°æ®å‡ºç°æ¦‚ç‡æœ€å¤§çš„å‚æ•°å€¼ã€‚

**English Explanation:**
Maximum likelihood estimation chooses parameter values that maximize the probability of observing the data.

**ğŸ”° é›¶åŸºç¡€ç†è§£ï¼šä»€ä¹ˆæ˜¯"ä¼¼ç„¶"ï¼Ÿ/ Zero-Basics: What is "Likelihood"?**

**é€šä¿—è§£é‡Š / Intuitive Explanation:**
æƒ³è±¡ä½ æœ‰ä¸€ä¸ª"é­”æ³•ç›’å­"ï¼Œé‡Œé¢æœ‰ä¸€äº›å‚æ•°ï¼ˆæ¯”å¦‚æ¦‚ç‡å€¼ï¼‰ã€‚ä½ å¾€ç›’å­é‡Œæ”¾æ•°æ®ï¼Œç›’å­ä¼šå‘Šè¯‰ä½ "è¿™äº›æ•°æ®å‡ºç°çš„å¯èƒ½æ€§æœ‰å¤šå¤§"ã€‚

Imagine you have a "magic box" with some parameters (like probability values). You put data into the box, and it tells you "how likely these data are to appear."

- **ä¼¼ç„¶ï¼ˆLikelihoodï¼‰** = åœ¨ç»™å®šå‚æ•°ä¸‹ï¼Œè§‚æµ‹åˆ°è¿™äº›æ•°æ®çš„"å¯èƒ½æ€§"
- **Likelihood** = The "possibility" of observing these data given the parameters
- **æœ€å¤§ä¼¼ç„¶ä¼°è®¡** = æ‰¾åˆ°è®©è¿™ä¸ª"å¯èƒ½æ€§"æœ€å¤§çš„å‚æ•°å€¼
- **Maximum Likelihood Estimation** = Find parameter values that make this "possibility" the largest

**ç”Ÿæ´»ç±»æ¯” / Life Analogy:**
- ä½ çœ‹åˆ°3ä¸ªè‹¹æœéƒ½æ˜¯çº¢è‰²çš„
- You see 3 apples, all are red
- å¦‚æœç›’å­å‚æ•°è¯´"çº¢è‹¹æœæ¦‚ç‡=0.9"ï¼Œé‚£ä¹ˆçœ‹åˆ°3ä¸ªçº¢è‹¹æœçš„å¯èƒ½æ€§ = 0.9 Ã— 0.9 Ã— 0.9 = 0.729
- If box parameters say "red apple probability = 0.9", then possibility of seeing 3 red apples = 0.9 Ã— 0.9 Ã— 0.9 = 0.729
- å¦‚æœç›’å­å‚æ•°è¯´"çº¢è‹¹æœæ¦‚ç‡=0.5"ï¼Œé‚£ä¹ˆå¯èƒ½æ€§ = 0.5 Ã— 0.5 Ã— 0.5 = 0.125
- If box parameters say "red apple probability = 0.5", then possibility = 0.5 Ã— 0.5 Ã— 0.5 = 0.125
- æ˜¾ç„¶0.9çš„å‚æ•°æ›´"åˆç†"ï¼ˆå› ä¸ºå®é™…çœ‹åˆ°3ä¸ªéƒ½æ˜¯çº¢çš„ï¼‰
- Obviously 0.9 parameter is more "reasonable" (because we actually see 3 red ones)

**ä¼¼ç„¶å‡½æ•° / Likelihood Function:**  
$$
L(\theta) = \prod_{i=1}^M P\big(x^{(i)}, y^{(i)} \mid \theta\big)
$$

**ğŸ”° é›¶åŸºç¡€ï¼šç¬¦å·è¯¦è§£ / Zero-Basics: Symbol Details**

**æ¯ä¸ªç¬¦å·çš„é€šä¿—è§£é‡Š / Intuitive Explanation of Each Symbol:**

1. **$L(\theta)$ - ä¼¼ç„¶å‡½æ•° / Likelihood Function**
   - **é€šä¿—ç†è§£**ï¼šä¸€ä¸ª"è¯„åˆ†å‡½æ•°"ï¼Œå‘Šè¯‰ä½ å‚æ•°Î¸æœ‰å¤š"å¥½"
   - **Intuitive**: A "scoring function" that tells how "good" parameter Î¸ is
   - **ä¾‹å­**ï¼šå¦‚æœL(Î¸â‚) = 0.8, L(Î¸â‚‚) = 0.3ï¼Œè¯´æ˜Î¸â‚æ¯”Î¸â‚‚æ›´å¥½
   - **Example**: If L(Î¸â‚) = 0.8, L(Î¸â‚‚) = 0.3, then Î¸â‚ is better than Î¸â‚‚

2. **$\prod_{i=1}^M$ - è¿ä¹˜ç¬¦å· / Product Symbol**
   - **é€šä¿—ç†è§£**ï¼šæŠŠæ‰€æœ‰ä¸œè¥¿"ä¹˜èµ·æ¥"
   - **Intuitive**: "Multiply" everything together
   - **ä¾‹å­**ï¼š$\prod_{i=1}^{3} a_i = a_1 Ã— a_2 Ã— a_3$
   - **Example**: $\prod_{i=1}^{3} a_i = a_1 Ã— a_2 Ã— a_3$
   - **ä¸ºä»€ä¹ˆç”¨ä¹˜ï¼Ÿ**ï¼šå› ä¸ºæ¯ä¸ªæ ·æœ¬æ˜¯"ç‹¬ç«‹"å‡ºç°çš„ï¼ˆä¸€ä¸ªå‡ºç°ä¸å½±å“å¦ä¸€ä¸ªï¼‰
   - **Why multiply?**: Because each sample appears "independently" (one doesn't affect another)

3. **$M$ - æ ·æœ¬æ•°é‡ / Number of Samples**
   - **é€šä¿—ç†è§£**ï¼šä½ æœ‰å¤šå°‘ä¸ªè®­ç»ƒæ•°æ®
   - **Intuitive**: How many training data you have
   - **ä¾‹å­**ï¼šM=3 è¡¨ç¤ºæœ‰3ä¸ªæ ·æœ¬
   - **Example**: M=3 means 3 samples

4. **$x^{(i)}$ - ç¬¬iä¸ªæ ·æœ¬çš„ç‰¹å¾ / Features of i-th Sample**
   - **é€šä¿—ç†è§£**ï¼šç¬¬iä¸ªæ ·æœ¬çš„"æè¿°ä¿¡æ¯"
   - **Intuitive**: "Description information" of i-th sample
   - **ä¾‹å­**ï¼šx^(1) = (1, 2) è¡¨ç¤ºç¬¬1ä¸ªæ ·æœ¬æœ‰2ä¸ªç‰¹å¾ï¼Œå€¼åˆ†åˆ«æ˜¯1å’Œ2
   - **Example**: x^(1) = (1, 2) means 1st sample has 2 features with values 1 and 2

5. **$y^{(i)}$ - ç¬¬iä¸ªæ ·æœ¬çš„æ ‡ç­¾ / Label of i-th Sample**
   - **é€šä¿—ç†è§£**ï¼šç¬¬iä¸ªæ ·æœ¬çš„"æ­£ç¡®ç­”æ¡ˆ"æˆ–"ç±»åˆ«"
   - **Intuitive**: "Correct answer" or "category" of i-th sample
   - **ä¾‹å­**ï¼šy^(1) = 1 è¡¨ç¤ºç¬¬1ä¸ªæ ·æœ¬å±äºç±»åˆ«1
   - **Example**: y^(1) = 1 means 1st sample belongs to category 1

6. **$\theta$ - æ¨¡å‹å‚æ•° / Model Parameters**
   - **é€šä¿—ç†è§£**ï¼šæ¨¡å‹çš„"è®¾ç½®"æˆ–"é…ç½®"
   - **Intuitive**: "Settings" or "configuration" of the model
   - **ä¾‹å­**ï¼šÎ¸å¯èƒ½åŒ…å«"ç±»åˆ«0çš„æ¦‚ç‡æ˜¯0.4"è¿™æ ·çš„ä¿¡æ¯
   - **Example**: Î¸ might contain information like "probability of class 0 is 0.4"

7. **$P(x^{(i)}, y^{(i)} \mid \theta)$ - è”åˆæ¦‚ç‡ / Joint Probability**
   - **é€šä¿—ç†è§£**ï¼šåœ¨å‚æ•°Î¸ä¸‹ï¼ŒåŒæ—¶çœ‹åˆ°ç‰¹å¾x^(i)å’Œæ ‡ç­¾y^(i)çš„æ¦‚ç‡
   - **Intuitive**: Probability of seeing both features x^(i) and label y^(i) under parameters Î¸
   - **ä¾‹å­**ï¼šP(x=(1,2), y=1 | Î¸) = 0.15 è¡¨ç¤ºåœ¨å‚æ•°Î¸ä¸‹ï¼Œçœ‹åˆ°ç‰¹å¾(1,2)ä¸”æ ‡ç­¾æ˜¯1çš„æ¦‚ç‡æ˜¯15%
   - **Example**: P(x=(1,2), y=1 | Î¸) = 0.15 means under Î¸, probability of seeing features (1,2) with label 1 is 15%

**ğŸ”° é›¶åŸºç¡€ï¼šä¸ºä»€ä¹ˆè¦ç›¸ä¹˜ï¼Ÿ/ Zero-Basics: Why Multiply?**

**ç›´è§‚ç†è§£ / Intuitive Understanding:**

æƒ³è±¡ä½ è¿ç»­æŠ›3æ¬¡ç¡¬å¸ï¼Œæ¯æ¬¡éƒ½æ˜¯æ­£é¢ï¼š
Imagine you flip a coin 3 times in a row, all are heads:

- ç¬¬1æ¬¡æ­£é¢æ¦‚ç‡ = 0.5
- ç¬¬2æ¬¡æ­£é¢æ¦‚ç‡ = 0.5ï¼ˆç‹¬ç«‹äº‹ä»¶ï¼Œä¸å—ç¬¬1æ¬¡å½±å“ï¼‰
- ç¬¬3æ¬¡æ­£é¢æ¦‚ç‡ = 0.5ï¼ˆç‹¬ç«‹äº‹ä»¶ï¼Œä¸å—å‰ä¸¤æ¬¡å½±å“ï¼‰

**3æ¬¡éƒ½æ˜¯æ­£é¢çš„æ¦‚ç‡ = 0.5 Ã— 0.5 Ã— 0.5 = 0.125**

**ä¸ºä»€ä¹ˆç›¸ä¹˜ï¼Ÿ/ Why Multiply?**
- å› ä¸ºæ¯æ¬¡æŠ›ç¡¬å¸æ˜¯"ç‹¬ç«‹äº‹ä»¶"ï¼ˆä¸€æ¬¡çš„ç»“æœä¸å½±å“å¦ä¸€æ¬¡ï¼‰
- Because each coin flip is an "independent event" (one result doesn't affect another)
- ç‹¬ç«‹äº‹ä»¶çš„è”åˆæ¦‚ç‡ = å„ä¸ªæ¦‚ç‡çš„ä¹˜ç§¯
- Joint probability of independent events = product of individual probabilities

**åœ¨æœºå™¨å­¦ä¹ ä¸­ / In Machine Learning:**
- æ¯ä¸ªè®­ç»ƒæ ·æœ¬å°±åƒä¸€æ¬¡"æŠ›ç¡¬å¸"
- Each training sample is like one "coin flip"
- æˆ‘ä»¬å‡è®¾æ ·æœ¬ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼ˆä¸€ä¸ªæ ·æœ¬ä¸å½±å“å¦ä¸€ä¸ªï¼‰
- We assume samples are independent (one doesn't affect another)
- æ‰€ä»¥æ‰€æœ‰æ ·æœ¬åŒæ—¶å‡ºç°çš„æ¦‚ç‡ = å„ä¸ªæ ·æœ¬æ¦‚ç‡çš„ä¹˜ç§¯
- So probability of all samples appearing = product of individual sample probabilities

**è®¡ç®—æ­¥éª¤ / Calculation Steps:**
1. å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬iï¼Œè®¡ç®— $P(x^{(i)}, y^{(i)} \mid \theta)$ / For each sample i, calculate $P(x^{(i)}, y^{(i)} \mid \theta)$
   - è¿™ä¸€æ­¥æ˜¯è®¡ç®—"å•ä¸ªæ ·æœ¬å‡ºç°çš„æ¦‚ç‡"
   - This step calculates "probability of a single sample appearing"
2. å°†æ‰€æœ‰æ¦‚ç‡ç›¸ä¹˜ / Multiply all probabilities together
   - è¿™ä¸€æ­¥æ˜¯è®¡ç®—"æ‰€æœ‰æ ·æœ¬åŒæ—¶å‡ºç°çš„æ¦‚ç‡"
   - This step calculates "probability of all samples appearing together"

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**

**æ­¥éª¤1ï¼šç†è§£æ•°æ® / Step 1: Understand the Data**

**ğŸ”° é›¶åŸºç¡€ï¼šä»€ä¹ˆæ˜¯"ç‰¹å¾"å’Œ"æ ‡ç­¾"ï¼Ÿ/ Zero-Basics: What are "Features" and "Labels"?**

**ç”Ÿæ´»ä¾‹å­ / Life Example:**
å‡è®¾ä½ è¦åˆ¤æ–­ä¸€ä¸ªæ°´æœæ˜¯"è‹¹æœ"è¿˜æ˜¯"æ©™å­"ï¼š
Suppose you want to determine if a fruit is "apple" or "orange":

- **ç‰¹å¾ï¼ˆFeaturesï¼‰**ï¼šä½ èƒ½è§‚å¯Ÿåˆ°çš„å±æ€§
  - **Features**: Attributes you can observe
  - æ¯”å¦‚ï¼šé¢œè‰²ã€å¤§å°ã€é‡é‡
  - E.g.: color, size, weight
- **æ ‡ç­¾ï¼ˆLabelï¼‰**ï¼šæ­£ç¡®ç­”æ¡ˆï¼ˆç±»åˆ«ï¼‰
  - **Label**: Correct answer (category)
  - æ¯”å¦‚ï¼šè‹¹æœ=1ï¼Œæ©™å­=0
  - E.g.: apple=1, orange=0

**æˆ‘ä»¬çš„ä¾‹å­ / Our Example:**
å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼ˆy âˆˆ {0, 1}ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰2ä¸ªç‰¹å¾ï¼ˆxâ‚, xâ‚‚ï¼‰
Suppose we have a binary classification problem (y âˆˆ {0, 1}), each sample has 2 features (xâ‚, xâ‚‚)

**è®­ç»ƒæ•°æ®é›†ï¼ˆå°±åƒä½ æ”¶é›†çš„æ ·æœ¬ï¼‰/ Training dataset (like samples you collected):**

| æ ·æœ¬ç¼–å· | ç‰¹å¾1 (xâ‚) | ç‰¹å¾2 (xâ‚‚) | æ ‡ç­¾ (y) | å«ä¹‰ |
| Sample # | Feature 1 (xâ‚) | Feature 2 (xâ‚‚) | Label (y) | Meaning |
|---------|------------|------------|---------|-------|
| æ ·æœ¬1 | 1 | 2 | 1 | ç‰¹å¾(1,2)å¯¹åº”ç±»åˆ«1 |
| Sample 1 | 1 | 2 | 1 | Features (1,2) correspond to class 1 |
| æ ·æœ¬2 | 2 | 3 | 1 | ç‰¹å¾(2,3)å¯¹åº”ç±»åˆ«1 |
| Sample 2 | 2 | 3 | 1 | Features (2,3) correspond to class 1 |
| æ ·æœ¬3 | 1 | 1 | 0 | ç‰¹å¾(1,1)å¯¹åº”ç±»åˆ«0 |
| Sample 3 | 1 | 1 | 0 | Features (1,1) correspond to class 0 |

**ç”¨æ•°å­¦ç¬¦å·è¡¨ç¤º / In Mathematical Notation:**
- æ ·æœ¬1 / Sample 1: x^(1) = (1, 2), y^(1) = 1
- æ ·æœ¬2 / Sample 2: x^(2) = (2, 3), y^(2) = 1  
- æ ·æœ¬3 / Sample 3: x^(3) = (1, 1), y^(3) = 0

**ğŸ”° ç†è§£è¦ç‚¹ / Key Points:**
- x^(1) ä¸­çš„ä¸Šæ ‡(1)è¡¨ç¤º"ç¬¬1ä¸ªæ ·æœ¬"ï¼Œä¸æ˜¯"1æ¬¡æ–¹"
- Superscript (1) in x^(1) means "1st sample", not "to the power of 1"
- (1, 2) è¡¨ç¤ºæœ‰2ä¸ªç‰¹å¾ï¼Œç¬¬ä¸€ä¸ªç‰¹å¾å€¼æ˜¯1ï¼Œç¬¬äºŒä¸ªæ˜¯2
- (1, 2) means 2 features, first feature value is 1, second is 2

**æ­¥éª¤2ï¼šç†è§£æœ´ç´ è´å¶æ–¯çš„å‡è®¾ / Step 2: Understand Naive Bayes Assumptions**

**ğŸ”° é›¶åŸºç¡€ï¼šä»€ä¹ˆæ˜¯"ç‹¬ç«‹"ï¼Ÿ/ Zero-Basics: What is "Independent"?**

**ç”Ÿæ´»ä¾‹å­ / Life Example:**
- **ä¸ç‹¬ç«‹**ï¼šä»Šå¤©ä¸‹é›¨ â†’ æ˜å¤©ä¹Ÿå¯èƒ½ä¸‹é›¨ï¼ˆæœ‰å…³è”ï¼‰
  - **Not independent**: It rains today â†’ It might rain tomorrow (related)
- **ç‹¬ç«‹**ï¼šä½ æŠ›ç¡¬å¸å¾—åˆ°æ­£é¢ â†’ ä¸å½±å“æˆ‘æŠ›ç¡¬å¸çš„ç»“æœï¼ˆæ— å…³è”ï¼‰
  - **Independent**: You flip coin get heads â†’ Doesn't affect my coin flip result (unrelated)

**åœ¨æœ´ç´ è´å¶æ–¯ä¸­ / In Naive Bayes:**
- æˆ‘ä»¬å‡è®¾**ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹**
- We assume **features are independent of each other**
- æ¯”å¦‚ï¼šç‰¹å¾1ï¼ˆé¢œè‰²ï¼‰å’Œç‰¹å¾2ï¼ˆå¤§å°ï¼‰äº’ä¸å½±å“
- E.g.: Feature 1 (color) and Feature 2 (size) don't affect each other

**ä¸ºä»€ä¹ˆå«"æœ´ç´ "ï¼Ÿ/ Why "Naive"?**
- å› ä¸ºç°å®ä¸­ç‰¹å¾å¾€å¾€æœ‰å…³è”ï¼ˆæ¯”å¦‚"çº¢è‰²"å’Œ"åœ†å½¢"åœ¨è‹¹æœä¸­ç»å¸¸ä¸€èµ·å‡ºç°ï¼‰
- Because in reality features are often related (e.g., "red" and "round" often appear together in apples)
- ä½†ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œæˆ‘ä»¬"æœ´ç´ åœ°"å‡è®¾å®ƒä»¬ç‹¬ç«‹
- But to simplify calculation, we "naively" assume they're independent

**æ•°å­¦å…¬å¼ / Mathematical Formula:**

$$P(x, y \mid \theta) = P(y \mid \theta) \times P(x_1 \mid y, \theta) \times P(x_2 \mid y, \theta) \times ... \times P(x_n \mid y, \theta)$$

**é€šä¿—è§£é‡Š / Intuitive Explanation:**
- å·¦è¾¹ï¼šçœ‹åˆ°ç‰¹å¾xå’Œæ ‡ç­¾yåŒæ—¶å‡ºç°çš„æ¦‚ç‡
- Left side: Probability of seeing features x and label y together
- å³è¾¹ï¼šç±»åˆ«æ¦‚ç‡ Ã— ç‰¹å¾1æ¦‚ç‡ Ã— ç‰¹å¾2æ¦‚ç‡ Ã— ...
- Right side: Class probability Ã— Feature 1 probability Ã— Feature 2 probability Ã— ...

**ä¸ºä»€ä¹ˆè¿™æ ·åˆ†è§£ï¼Ÿ/ Why This Decomposition?**
- å› ä¸ºå‡è®¾ç‰¹å¾ç‹¬ç«‹ï¼Œæ‰€ä»¥è”åˆæ¦‚ç‡ = å„ä¸ªæ¦‚ç‡çš„ä¹˜ç§¯
- Because features are assumed independent, joint probability = product of individual probabilities

**ç¬¦å·è¯´æ˜ / Symbol Explanation:**
- $P(y \mid \theta)$ï¼šç±»åˆ«yçš„å…ˆéªŒæ¦‚ç‡ï¼ˆ"å…ˆéªŒ"=åœ¨è§‚å¯Ÿç‰¹å¾ä¹‹å‰å°±çŸ¥é“çš„ï¼‰
  - **Prior probability** of class y ("prior" = known before observing features)
  - æ¯”å¦‚ï¼šåœ¨ä¸çŸ¥é“ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ ·æœ¬å±äºç±»åˆ«1çš„æ¦‚ç‡æ˜¯60%
  - E.g.: Without knowing features, probability of sample belonging to class 1 is 60%
- $P(x_j \mid y, \theta)$ï¼šåœ¨ç±»åˆ«yä¸‹ï¼Œç¬¬jä¸ªç‰¹å¾çš„æ¡ä»¶æ¦‚ç‡
  - **Conditional probability** of j-th feature given class y
  - æ¯”å¦‚ï¼šå¦‚æœå·²çŸ¥æ˜¯ç±»åˆ«1ï¼Œé‚£ä¹ˆç‰¹å¾1=1çš„æ¦‚ç‡æ˜¯50%
  - E.g.: If known to be class 1, then probability of feature 1=1 is 50%
  
  **ğŸ”° é›¶åŸºç¡€ï¼šä»€ä¹ˆæ˜¯"ç‰¹å¾1=1"ï¼Ÿ/ Zero-Basics: What is "Feature 1=1"?**
  
  **è¯¦ç»†è§£é‡Š / Detailed Explanation:**
  - **"ç‰¹å¾1"** = ç¬¬1ä¸ªç‰¹å¾ï¼ˆç¬¬ä¸€ä¸ªç‰¹å¾ï¼Œä¸æ˜¯"1æ¬¡æ–¹"ï¼‰
  - **"Feature 1"** = The 1st feature (first feature, not "to the power of 1")
  - **"=1"** = è¿™ä¸ªç‰¹å¾çš„å€¼ä¸º1
  - **"=1"** = The value of this feature is 1
  - **"ç‰¹å¾1=1"** = ç¬¬1ä¸ªç‰¹å¾çš„å€¼ä¸º1
  - **"Feature 1=1"** = The 1st feature has value 1
  
  **å…·ä½“ä¾‹å­ / Concrete Example:**
  å‡è®¾ä¸€ä¸ªæ ·æœ¬æœ‰2ä¸ªç‰¹å¾ï¼š
  Suppose a sample has 2 features:
  - æ ·æœ¬ï¼šx = (1, 2)
  - Sample: x = (1, 2)
  - è¿™é‡Œï¼šç‰¹å¾1 = 1ï¼Œç‰¹å¾2 = 2
  - Here: Feature 1 = 1, Feature 2 = 2
  
  **æ¦‚ç‡çš„å«ä¹‰ / Meaning of Probability:**
  - P(ç‰¹å¾1=1 | y=1) = 0.5 çš„æ„æ€æ˜¯ï¼š
  - P(feature 1=1 | y=1) = 0.5 means:
  - åœ¨**ç±»åˆ«1**çš„æ‰€æœ‰æ ·æœ¬ä¸­ï¼Œæœ‰50%çš„æ ·æœ¬å…¶**ç¬¬1ä¸ªç‰¹å¾çš„å€¼æ˜¯1**
  - Among all samples of **class 1**, 50% have **feature 1 with value 1**
  
  **ç”¨è¡¨æ ¼ç†è§£ / Understanding with Table:**
  
  | æ ·æœ¬ç¼–å· | ç‰¹å¾1çš„å€¼ | ç‰¹å¾2çš„å€¼ | ç±»åˆ« |
  | Sample # | Feature 1 Value | Feature 2 Value | Class |
  |---------|-------------|-------------|------|
  | æ ·æœ¬1 | **1** | 2 | 1 |
  | Sample 1 | **1** | 2 | 1 |
  | æ ·æœ¬2 | **1** | 3 | 1 |
  | Sample 2 | **1** | 3 | 1 |
  | æ ·æœ¬3 | 2 | 2 | 1 |
  | Sample 3 | 2 | 2 | 1 |
  | æ ·æœ¬4 | 2 | 3 | 1 |
  | Sample 4 | 2 | 3 | 1 |
  
  - åœ¨ç±»åˆ«1çš„4ä¸ªæ ·æœ¬ä¸­ï¼Œæœ‰2ä¸ªæ ·æœ¬çš„ç‰¹å¾1=1ï¼ˆæ ·æœ¬1å’Œæ ·æœ¬2ï¼‰
  - Among 4 samples of class 1, 2 have feature 1=1 (sample 1 and sample 2)
  - æ‰€ä»¥ P(ç‰¹å¾1=1 | y=1) = 2/4 = 0.5 = 50%
  - So P(feature 1=1 | y=1) = 2/4 = 0.5 = 50%

**æ­¥éª¤3ï¼šå‡è®¾å·²çŸ¥çš„å‚æ•°Î¸ / Step 3: Assume Known Parameters Î¸**

**ğŸ”° é›¶åŸºç¡€ï¼šå‚æ•°æ˜¯ä»€ä¹ˆï¼Ÿ/ Zero-Basics: What are Parameters?**

**é€šä¿—ç†è§£ / Intuitive Understanding:**
å‚æ•°å°±åƒ"è§„åˆ™è¡¨"ï¼Œå‘Šè¯‰ä½ åœ¨ä¸åŒæƒ…å†µä¸‹æ¦‚ç‡æ˜¯å¤šå°‘
Parameters are like "rule tables" that tell you probabilities in different situations

**ç±»åˆ«å…ˆéªŒæ¦‚ç‡ / Class Prior Probabilities:**
- **å«ä¹‰**ï¼šåœ¨ä¸çŸ¥é“ä»»ä½•ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ ·æœ¬å±äºå„ç±»åˆ«çš„æ¦‚ç‡
- **Meaning**: Probability of a sample belonging to each class without knowing any features
- P(y=0) = 0.4 â†’ 40%çš„æ ·æœ¬å±äºç±»åˆ«0
- P(y=0) = 0.4 â†’ 40% of samples belong to class 0
- P(y=1) = 0.6 â†’ 60%çš„æ ·æœ¬å±äºç±»åˆ«1
- P(y=1) = 0.6 â†’ 60% of samples belong to class 1
- **éªŒè¯**ï¼š0.4 + 0.6 = 1.0 âœ“ï¼ˆæ‰€æœ‰ç±»åˆ«æ¦‚ç‡åŠ èµ·æ¥ç­‰äº1ï¼‰
- **Check**: 0.4 + 0.6 = 1.0 âœ“ (All class probabilities sum to 1)

**ç‰¹å¾æ¡ä»¶æ¦‚ç‡è¡¨ / Feature Conditional Probability Tables:**

**å¯¹äºç±»åˆ«y=0 / For class y=0:**

| ç‰¹å¾ | ç‰¹å¾å€¼ | æ¦‚ç‡ | å«ä¹‰ |
| Feature | Value | Probability | Meaning |
|------|------|-----------|-------|
| xâ‚ | 1 | 0.7 | åœ¨ç±»åˆ«0ä¸­ï¼Œ70%çš„æ ·æœ¬ç‰¹å¾1=1 |
| xâ‚ | 1 | 0.7 | In class 0, 70% of samples have feature 1=1 |
| xâ‚ | 2 | 0.3 | åœ¨ç±»åˆ«0ä¸­ï¼Œ30%çš„æ ·æœ¬ç‰¹å¾1=2 |
| xâ‚ | 2 | 0.3 | In class 0, 30% of samples have feature 1=2 |
| xâ‚‚ | 1 | 0.8 | åœ¨ç±»åˆ«0ä¸­ï¼Œ80%çš„æ ·æœ¬ç‰¹å¾2=1 |
| xâ‚‚ | 1 | 0.8 | In class 0, 80% of samples have feature 2=1 |
| xâ‚‚ | 2 | 0.2 | åœ¨ç±»åˆ«0ä¸­ï¼Œ20%çš„æ ·æœ¬ç‰¹å¾2=2 |
| xâ‚‚ | 2 | 0.2 | In class 0, 20% of samples have feature 2=2 |
| xâ‚‚ | 3 | 0.0 | åœ¨ç±»åˆ«0ä¸­ï¼Œ0%çš„æ ·æœ¬ç‰¹å¾2=3ï¼ˆä¸å¯èƒ½ï¼‰ |
| xâ‚‚ | 3 | 0.0 | In class 0, 0% of samples have feature 2=3 (impossible) |

**éªŒè¯**ï¼šå¯¹äºxâ‚ï¼Œ0.7 + 0.3 = 1.0 âœ“ï¼›å¯¹äºxâ‚‚ï¼Œ0.8 + 0.2 + 0.0 = 1.0 âœ“
**Check**: For xâ‚, 0.7 + 0.3 = 1.0 âœ“; For xâ‚‚, 0.8 + 0.2 + 0.0 = 1.0 âœ“

**å¯¹äºç±»åˆ«y=1 / For class y=1:**

| ç‰¹å¾ | ç‰¹å¾å€¼ | æ¦‚ç‡ | å«ä¹‰ |
| Feature | Value | Probability | Meaning |
|------|------|-----------|-------|
| xâ‚ | 1 | 0.5 | åœ¨ç±»åˆ«1ä¸­ï¼Œ50%çš„æ ·æœ¬ç‰¹å¾1=1 |
| xâ‚ | 1 | 0.5 | In class 1, 50% of samples have feature 1=1 |
| xâ‚ | 2 | 0.5 | åœ¨ç±»åˆ«1ä¸­ï¼Œ50%çš„æ ·æœ¬ç‰¹å¾1=2 |
| xâ‚ | 2 | 0.5 | In class 1, 50% of samples have feature 1=2 |
| xâ‚‚ | 1 | 0.2 | åœ¨ç±»åˆ«1ä¸­ï¼Œ20%çš„æ ·æœ¬ç‰¹å¾2=1 |
| xâ‚‚ | 1 | 0.2 | In class 1, 20% of samples have feature 2=1 |
| xâ‚‚ | 2 | 0.5 | åœ¨ç±»åˆ«1ä¸­ï¼Œ50%çš„æ ·æœ¬ç‰¹å¾2=2 |
| xâ‚‚ | 2 | 0.5 | In class 1, 50% of samples have feature 2=2 |
| xâ‚‚ | 3 | 0.3 | åœ¨ç±»åˆ«1ä¸­ï¼Œ30%çš„æ ·æœ¬ç‰¹å¾2=3 |
| xâ‚‚ | 3 | 0.3 | In class 1, 30% of samples have feature 2=3 |

**éªŒè¯**ï¼šå¯¹äºxâ‚ï¼Œ0.5 + 0.5 = 1.0 âœ“ï¼›å¯¹äºxâ‚‚ï¼Œ0.2 + 0.5 + 0.3 = 1.0 âœ“
**Check**: For xâ‚, 0.5 + 0.5 = 1.0 âœ“; For xâ‚‚, 0.2 + 0.5 + 0.3 = 1.0 âœ“

**ğŸ”° è¿™äº›å‚æ•°ä»å“ªé‡Œæ¥ï¼Ÿ/ Where Do These Parameters Come From?**
- é€šå¸¸ä»è®­ç»ƒæ•°æ®ä¸­"å­¦ä¹ "æˆ–"ä¼°è®¡"å¾—åˆ°
- Usually "learned" or "estimated" from training data
- æ¯”å¦‚ï¼šå¦‚æœè®­ç»ƒæ•°æ®ä¸­60%çš„æ ·æœ¬æ˜¯ç±»åˆ«1ï¼Œé‚£ä¹ˆP(y=1) = 0.6
- E.g.: If 60% of training samples are class 1, then P(y=1) = 0.6
- è¿™é‡Œæˆ‘ä»¬å‡è®¾å·²ç»çŸ¥é“äº†è¿™äº›å‚æ•°ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å…ˆä¼°è®¡ï¼‰
- Here we assume we already know these parameters (in practice, we need to estimate them first)

**æ­¥éª¤4ï¼šè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è”åˆæ¦‚ç‡ / Step 4: Calculate Joint Probability for Each Sample**

**æ ·æœ¬1: x=(1, 2), y=1**

**ğŸ”° é›¶åŸºç¡€ï¼šé€æ­¥è®¡ç®— / Zero-Basics: Step-by-Step Calculation**

**æ­¥éª¤1ï¼šç†è§£é—®é¢˜ / Step 1: Understand the Problem**
- æˆ‘ä»¬è¦è®¡ç®—ï¼šåœ¨å‚æ•°Î¸ä¸‹ï¼Œçœ‹åˆ°ç‰¹å¾(1,2)ä¸”æ ‡ç­¾æ˜¯1çš„æ¦‚ç‡
- We want to calculate: Under parameters Î¸, probability of seeing features (1,2) with label 1

**æ­¥éª¤2ï¼šåº”ç”¨æœ´ç´ è´å¶æ–¯å…¬å¼ / Step 2: Apply Naive Bayes Formula**

æ ¹æ®æœ´ç´ è´å¶æ–¯çš„ç‹¬ç«‹å‡è®¾ï¼š
According to Naive Bayes independence assumption:

$$P(x=(1,2), y=1 \mid \theta) = P(y=1) \times P(x_1=1 \mid y=1) \times P(x_2=2 \mid y=1)$$

**é€šä¿—è§£é‡Š / Intuitive Explanation:**
- è¿™ä¸ªå…¬å¼è¯´ï¼šè¦åŒæ—¶çœ‹åˆ°ç±»åˆ«1ã€ç‰¹å¾1=1ã€ç‰¹å¾2=2ï¼Œéœ€è¦ï¼š
- This formula says: To see class 1, feature 1=1, and feature 2=2 together, we need:
  1. é¦–å…ˆæ˜¯ç±»åˆ«1ï¼ˆæ¦‚ç‡0.6ï¼‰
  1. First be class 1 (probability 0.6)
  2. åœ¨ç±»åˆ«1ä¸­ï¼Œç‰¹å¾1=1ï¼ˆæ¦‚ç‡0.5ï¼‰
  2. In class 1, feature 1=1 (probability 0.5)
  3. åœ¨ç±»åˆ«1ä¸­ï¼Œç‰¹å¾2=2ï¼ˆæ¦‚ç‡0.5ï¼‰
  3. In class 1, feature 2=2 (probability 0.5)
- å› ä¸ºå‡è®¾ç‹¬ç«‹ï¼Œæ‰€ä»¥æ¦‚ç‡ç›¸ä¹˜
- Because of independence assumption, probabilities multiply

**æ­¥éª¤3ï¼šæŸ¥æ‰¾å‚æ•°è¡¨ / Step 3: Look Up Parameter Tables**

ä»æ­¥éª¤3çš„å‚æ•°è¡¨ä¸­æŸ¥æ‰¾ï¼š
Look up from parameter tables in Step 3:

- P(y=1) = 0.6ï¼ˆç±»åˆ«å…ˆéªŒæ¦‚ç‡è¡¨ï¼‰
- P(y=1) = 0.6 (from class prior probability table)
- P(xâ‚=1 | y=1) = 0.5ï¼ˆç±»åˆ«1çš„ç‰¹å¾1æ¡ä»¶æ¦‚ç‡è¡¨ï¼‰
- P(xâ‚=1 | y=1) = 0.5 (from class 1's feature 1 conditional probability table)
- P(xâ‚‚=2 | y=1) = 0.5ï¼ˆç±»åˆ«1çš„ç‰¹å¾2æ¡ä»¶æ¦‚ç‡è¡¨ï¼‰
- P(xâ‚‚=2 | y=1) = 0.5 (from class 1's feature 2 conditional probability table)

**æ­¥éª¤4ï¼šä»£å…¥è®¡ç®— / Step 4: Substitute and Calculate**

$$P(x=(1,2), y=1 \mid \theta) = 0.6 \times 0.5 \times 0.5$$

**è¯¦ç»†è®¡ç®—è¿‡ç¨‹ / Detailed Calculation Process:**
- 0.6 Ã— 0.5 = 0.3ï¼ˆå…ˆç®—å‰ä¸¤é¡¹ï¼‰
- 0.6 Ã— 0.5 = 0.3 (calculate first two terms)
- 0.3 Ã— 0.5 = 0.15ï¼ˆå†ä¹˜ä»¥ç¬¬ä¸‰é¡¹ï¼‰
- 0.3 Ã— 0.5 = 0.15 (multiply by third term)

**ç»“æœ / Result:**
$$P(x=(1,2), y=1 \mid \theta) = 0.15$$

**å«ä¹‰ / Meaning:**
- åœ¨å‚æ•°Î¸ä¸‹ï¼Œçœ‹åˆ°æ ·æœ¬(ç‰¹å¾1=1, ç‰¹å¾2=2, æ ‡ç­¾=1)çš„æ¦‚ç‡æ˜¯15%
- Under parameters Î¸, probability of seeing sample (feature 1=1, feature 2=2, label=1) is 15%

**æ ·æœ¬2: x=(2, 3), y=1**
- $$P(x=(2,3), y=1 \mid \theta) = P(y=1) \times P(x_1=2 \mid y=1) \times P(x_2=3 \mid y=1)$$
- $$P(x=(2,3), y=1 \mid \theta) = 0.6 \times 0.5 \times 0.3 = 0.09$$

**æ ·æœ¬3: x=(1, 1), y=0**
- $$P(x=(1,1), y=0 \mid \theta) = P(y=0) \times P(x_1=1 \mid y=0) \times P(x_2=1 \mid y=0)$$
- $$P(x=(1,1), y=0 \mid \theta) = 0.4 \times 0.7 \times 0.8 = 0.224$$

**æ­¥éª¤5ï¼šè®¡ç®—ä¼¼ç„¶å‡½æ•° / Step 5: Calculate Likelihood Function**

**ğŸ”° é›¶åŸºç¡€ï¼šä»€ä¹ˆæ˜¯"æ‰€æœ‰æ ·æœ¬åŒæ—¶å‡ºç°"ï¼Ÿ/ Zero-Basics: What is "All Samples Appear Together"?**

**ç”Ÿæ´»ç±»æ¯” / Life Analogy:**
- ä½ è¿ç»­æŠ›3æ¬¡ç¡¬å¸ï¼Œæƒ³çŸ¥é“"3æ¬¡éƒ½æ˜¯æ­£é¢"çš„æ¦‚ç‡
- You flip a coin 3 times, want to know probability of "all 3 are heads"
- è¿™å°±æ˜¯"æ‰€æœ‰äº‹ä»¶åŒæ—¶å‘ç”Ÿ"çš„æ¦‚ç‡
- This is probability of "all events happening together"

**åœ¨æœºå™¨å­¦ä¹ ä¸­ / In Machine Learning:**
- æˆ‘ä»¬æƒ³çŸ¥é“ï¼šåœ¨å‚æ•°Î¸ä¸‹ï¼Œ**åŒæ—¶çœ‹åˆ°è¿™3ä¸ªè®­ç»ƒæ ·æœ¬**çš„æ¦‚ç‡
- We want to know: Under parameters Î¸, probability of **seeing all 3 training samples together**
- è¿™å°±æ˜¯"ä¼¼ç„¶å‡½æ•°"çš„å«ä¹‰
- This is what "likelihood function" means

**æ•°å­¦å…¬å¼ / Mathematical Formula:**

$$L(\theta) = \prod_{i=1}^{3} P(x^{(i)}, y^{(i)} \mid \theta)$$

**å±•å¼€å½¢å¼ / Expanded Form:**

$$L(\theta) = P(x^{(1)}, y^{(1)} \mid \theta) \times P(x^{(2)}, y^{(2)} \mid \theta) \times P(x^{(3)}, y^{(3)} \mid \theta)$$

**ä»£å…¥æˆ‘ä»¬è®¡ç®—å‡ºçš„å€¼ / Substitute Our Calculated Values:**

ä»å‰é¢æ­¥éª¤æˆ‘ä»¬çŸ¥é“ï¼š
From previous steps we know:
- P(x^(1), y^(1)|Î¸) = 0.15ï¼ˆæ ·æœ¬1çš„æ¦‚ç‡ï¼‰
- P(x^(1), y^(1)|Î¸) = 0.15 (probability of sample 1)
- P(x^(2), y^(2)|Î¸) = 0.09ï¼ˆæ ·æœ¬2çš„æ¦‚ç‡ï¼‰
- P(x^(2), y^(2)|Î¸) = 0.09 (probability of sample 2)
- P(x^(3), y^(3)|Î¸) = 0.224ï¼ˆæ ·æœ¬3çš„æ¦‚ç‡ï¼‰
- P(x^(3), y^(3)|Î¸) = 0.224 (probability of sample 3)

**è¯¦ç»†è®¡ç®—è¿‡ç¨‹ / Detailed Calculation Process:**

$$L(\theta) = 0.15 \times 0.09 \times 0.224$$

**åˆ†æ­¥è®¡ç®— / Step-by-Step:**
1. å…ˆç®—å‰ä¸¤é¡¹ï¼š0.15 Ã— 0.09 = 0.0135
1. Calculate first two: 0.15 Ã— 0.09 = 0.0135
2. å†ä¹˜ä»¥ç¬¬ä¸‰é¡¹ï¼š0.0135 Ã— 0.224 = 0.003024
2. Multiply by third: 0.0135 Ã— 0.224 = 0.003024

**æœ€ç»ˆç»“æœ / Final Result:**

$$L(\theta) = 0.003024$$

**ğŸ”° è¿™ä¸ªæ•°å­—å¾ˆå°ï¼Œæ­£å¸¸å—ï¼Ÿ/ Is This Small Number Normal?**

**æ˜¯çš„ï¼Œå®Œå…¨æ­£å¸¸ï¼/ Yes, completely normal!**

**åŸå›  / Reason:**
- è¿™æ˜¯3ä¸ªæ¦‚ç‡çš„ä¹˜ç§¯ï¼Œæ¯ä¸ªæ¦‚ç‡éƒ½å°äº1
- This is product of 3 probabilities, each less than 1
- å¤šä¸ªå°äº1çš„æ•°ç›¸ä¹˜ï¼Œç»“æœä¼šè¶Šæ¥è¶Šå°
- Multiplying numbers less than 1 makes result smaller and smaller
- æ¯”å¦‚ï¼š0.5 Ã— 0.5 Ã— 0.5 = 0.125ï¼ˆå·²ç»å¾ˆå°äº†ï¼‰
- E.g.: 0.5 Ã— 0.5 Ã— 0.5 = 0.125 (already very small)

**å®é™…æ„ä¹‰ / Practical Meaning:**
- L(Î¸) = 0.003024 è¡¨ç¤ºï¼šåœ¨å‚æ•°Î¸ä¸‹ï¼ŒåŒæ—¶çœ‹åˆ°è¿™3ä¸ªç‰¹å®šæ ·æœ¬çš„æ¦‚ç‡æ˜¯0.3024%
- L(Î¸) = 0.003024 means: Under parameters Î¸, probability of seeing these 3 specific samples together is 0.3024%
- è¿™ä¸ªæ¦‚ç‡å¾ˆå°æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸º"æ°å¥½æ˜¯è¿™3ä¸ªæ ·æœ¬"æ˜¯ä¸€ä¸ªå¾ˆå…·ä½“çš„äº‹ä»¶
- This small probability is normal, because "exactly these 3 samples" is a very specific event

**æ­¥éª¤6ï¼šè§£é‡Šç»“æœ / Step 6: Interpret the Result**

**ğŸ”° é›¶åŸºç¡€ï¼šè¿™ä¸ªç»“æœå‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ/ Zero-Basics: What Does This Result Tell Us?**

**1. ä¼¼ç„¶å€¼çš„å«ä¹‰ / Meaning of Likelihood Value**
- L(Î¸) = 0.003024 è¡¨ç¤ºï¼šåœ¨å‚æ•°Î¸ä¸‹ï¼Œè§‚æµ‹åˆ°è¿™3ä¸ªæ ·æœ¬çš„è”åˆæ¦‚ç‡æ˜¯0.3024%
- L(Î¸) = 0.003024 means: Under parameters Î¸, joint probability of observing these 3 samples is 0.3024%
- æ¢å¥è¯è¯´ï¼šå¦‚æœå‚æ•°Î¸æ˜¯æ­£ç¡®çš„ï¼Œé‚£ä¹ˆçœ‹åˆ°è¿™3ä¸ªæ ·æœ¬çš„å¯èƒ½æ€§æ˜¯0.3024%
- In other words: If parameters Î¸ are correct, then possibility of seeing these 3 samples is 0.3024%

**2. ä¸ºä»€ä¹ˆå€¼å¾ˆå°ï¼Ÿ/ Why Is the Value Small?**
- **å®Œå…¨æ­£å¸¸ï¼** å› ä¸ºï¼š
- **Completely normal!** Because:
  - è¿™æ˜¯å¤šä¸ªæ¦‚ç‡çš„ä¹˜ç§¯ï¼ˆæ¯ä¸ªéƒ½<1ï¼‰
  - It's product of multiple probabilities (each <1)
  - æ ·æœ¬è¶Šå¤šï¼Œä¹˜ç§¯è¶Šå°
  - More samples, smaller product
  - å¦‚æœæœ‰100ä¸ªæ ·æœ¬ï¼Œç»“æœå¯èƒ½æ˜¯10^(-50)è¿™æ ·æå°çš„æ•°
  - If there are 100 samples, result might be extremely small like 10^(-50)

**3. æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç›®æ ‡ / Goal of Maximum Likelihood Estimation**

**æ ¸å¿ƒæ€æƒ³ / Core Idea:**
- æˆ‘ä»¬æƒ³è¦æ‰¾åˆ°**æœ€å¥½çš„å‚æ•°Î¸**ï¼Œä½¿å¾—L(Î¸)æœ€å¤§
- We want to find **best parameters Î¸** that maximize L(Î¸)
- ä¹Ÿå°±æ˜¯è¯´ï¼šæ‰¾åˆ°è®©"çœ‹åˆ°è¿™äº›æ•°æ®"æœ€å¯èƒ½çš„å‚æ•°
- That is: Find parameters that make "seeing these data" most likely

**ä¾‹å­ / Example:**
å‡è®¾æˆ‘ä»¬å°è¯•ä¸¤ç»„å‚æ•°ï¼š
Suppose we try two sets of parameters:

| å‚æ•°ç»„ | L(Î¸)å€¼ | è¯„ä»· |
| Parameter Set | L(Î¸) Value | Evaluation |
|--------|---------|------|
| Î¸â‚ | 0.003024 | å½“å‰å‚æ•° |
| Î¸â‚ | 0.003024 | Current parameters |
| Î¸â‚‚ | 0.001000 | æ›´å·®ï¼ˆå¯èƒ½æ€§æ›´å°ï¼‰ |
| Î¸â‚‚ | 0.001000 | Worse (less likely) |
| Î¸â‚ƒ | 0.005000 | æ›´å¥½ï¼ˆå¯èƒ½æ€§æ›´å¤§ï¼‰ |
| Î¸â‚ƒ | 0.005000 | Better (more likely) |

- å¦‚æœL(Î¸â‚ƒ) > L(Î¸â‚)ï¼Œè¯´æ˜Î¸â‚ƒæ¯”Î¸â‚æ›´å¥½
- If L(Î¸â‚ƒ) > L(Î¸â‚), then Î¸â‚ƒ is better than Î¸â‚
- æœ€å¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯å¯»æ‰¾ä½¿L(Î¸)æœ€å¤§çš„Î¸
- Maximum likelihood estimation seeks Î¸ that maximizes L(Î¸)

**4. å®é™…åº”ç”¨ / Practical Application**

åœ¨å®é™…ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ï¼š
In practice, we usually:
1. ä»è®­ç»ƒæ•°æ®ä¼°è®¡å‚æ•°Î¸ï¼ˆæ¯”å¦‚è®¡ç®—å„ç±»åˆ«çš„é¢‘ç‡ï¼‰
1. Estimate parameters Î¸ from training data (e.g., calculate frequencies of each class)
2. è®¡ç®—ä¼¼ç„¶å‡½æ•°L(Î¸)
2. Calculate likelihood function L(Î¸)
3. è°ƒæ•´å‚æ•°ä½¿L(Î¸)æœ€å¤§ï¼ˆè¿™å°±æ˜¯"å­¦ä¹ "è¿‡ç¨‹ï¼‰
3. Adjust parameters to maximize L(Î¸) (this is the "learning" process)

**å¯¹æ•°ä¼¼ç„¶ / Log-Likelihood:**  
$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^M \log P\big(x^{(i)}, y^{(i)} \mid \theta\big)
$$

**ç¬¦å·è¯´æ˜ / Symbol Explanation:**
- $\ell(\theta)$ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•° / Log-likelihood function
- $\log$ï¼šè‡ªç„¶å¯¹æ•°ï¼ˆæˆ–å¸¸ç”¨å¯¹æ•°ï¼‰/ Natural logarithm (or common logarithm)
- $\sum_{i=1}^M$ï¼šä»i=1åˆ°Mçš„æ±‚å’Œç¬¦å· / Sum from i=1 to M

**è®¡ç®—æ­¥éª¤ / Calculation Steps:**
1. å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬iï¼Œè®¡ç®— $\log P(x^{(i)}, y^{(i)} \mid \theta)$ / For each sample i, calculate $\log P(x^{(i)}, y^{(i)} \mid \theta)$
2. å°†æ‰€æœ‰å¯¹æ•°æ¦‚ç‡ç›¸åŠ  / Sum all log probabilities
3. ä¼˜ç‚¹ï¼šå°†ä¹˜æ³•å˜ä¸ºåŠ æ³•ï¼Œé¿å…æ•°å€¼ä¸‹æº¢ / Advantage: converts multiplication to addition, avoids numerical underflow

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
ä½¿ç”¨ä¸Šé¢è¯¦ç»†ä¾‹å­ä¸­çš„æ¦‚ç‡å€¼ï¼š
Using probability values from the detailed example above:

**æ­¥éª¤1ï¼šè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡ / Step 1: Calculate Log Probability for Each Sample**

- log P(x^(1), y^(1)|Î¸) = log(0.15) â‰ˆ -1.897
- log P(x^(2), y^(2)|Î¸) = log(0.09) â‰ˆ -2.408
- log P(x^(3), y^(3)|Î¸) = log(0.224) â‰ˆ -1.495

**æ­¥éª¤2ï¼šè®¡ç®—å¯¹æ•°ä¼¼ç„¶ / Step 2: Calculate Log-Likelihood**

å¯¹æ•°ä¼¼ç„¶æ˜¯å¯¹æ•°æ¦‚ç‡çš„å’Œï¼š
Log-likelihood is the sum of log probabilities:

$$\ell(\theta) = \sum_{i=1}^{3} \log P(x^{(i)}, y^{(i)} \mid \theta)$$

$$\ell(\theta) = \log(0.15) + \log(0.09) + \log(0.224)$$

$$\ell(\theta) = -1.897 + (-2.408) + (-1.495)$$

$$\ell(\theta) = -5.800$$

**æ­¥éª¤3ï¼šéªŒè¯ / Step 3: Verification**

éªŒè¯å¯¹æ•°ä¼¼ç„¶ä¸ä¼¼ç„¶å‡½æ•°çš„å…³ç³»ï¼š
Verify relationship between log-likelihood and likelihood:

$$\ell(\theta) = \log L(\theta) = \log(0.003024) \approx -5.800$$

âœ“ éªŒè¯é€šè¿‡ / Verification passed

**æ­¥éª¤4ï¼šä¸ºä»€ä¹ˆä½¿ç”¨å¯¹æ•°ä¼¼ç„¶ï¼Ÿ/ Step 4: Why Use Log-Likelihood?**

**ä¼˜ç‚¹1ï¼šæ•°å€¼ç¨³å®šæ€§ / Advantage 1: Numerical Stability**
- å½“æ¦‚ç‡å¾ˆå°æ—¶ï¼ˆå¦‚0.0001ï¼‰ï¼Œç›´æ¥ç›¸ä¹˜å¯èƒ½ä¸‹æº¢ï¼ˆè®¡ç®—æœºæ— æ³•è¡¨ç¤ºï¼‰
- When probabilities are very small (e.g., 0.0001), direct multiplication may underflow (computer cannot represent)
- ä¾‹å¦‚ï¼š0.0001 Ã— 0.0001 Ã— 0.0001 = 1e-12ï¼ˆå¯èƒ½ä¸‹æº¢ï¼‰
- Example: 0.0001 Ã— 0.0001 Ã— 0.0001 = 1e-12 (may underflow)
- ä½†å¯¹æ•°ç›¸åŠ ï¼šlog(0.0001) + log(0.0001) + log(0.0001) = -9.21 - 9.21 - 9.21 = -27.63ï¼ˆç¨³å®šï¼‰
- But log addition: log(0.0001) + log(0.0001) + log(0.0001) = -9.21 - 9.21 - 9.21 = -27.63 (stable)

**ä¼˜ç‚¹2ï¼šè®¡ç®—ç®€åŒ– / Advantage 2: Computational Simplification**
- ä¹˜æ³•å˜ä¸ºåŠ æ³•ï¼Œè®¡ç®—æ›´å¿«
- Multiplication becomes addition, faster computation
- æ±‚å¯¼æ›´ç®€å•ï¼ˆå¯¹æ•°å’Œæ±‚å¯¼æ¯”ä¹˜ç§¯æ±‚å¯¼ç®€å•ï¼‰
- Derivatives are simpler (logarithm derivatives are simpler than product derivatives)

**ä¼˜ç‚¹3ï¼šä¼˜åŒ–æ–¹ä¾¿ / Advantage 3: Optimization Convenience**
- æœ€å¤§åŒ–L(Î¸)ç­‰ä»·äºæœ€å¤§åŒ–â„“(Î¸)ï¼ˆå› ä¸ºlogæ˜¯å•è°ƒé€’å¢å‡½æ•°ï¼‰
- Maximizing L(Î¸) is equivalent to maximizing â„“(Î¸) (because log is monotonic increasing)
- ä½†é€šå¸¸æˆ‘ä»¬æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ -â„“(Î¸)ï¼ˆè½¬æ¢ä¸ºæœ€å°åŒ–é—®é¢˜ï¼‰
- But usually we minimize negative log-likelihood -â„“(Î¸) (convert to minimization problem)

---

## ğŸ”° é›¶åŸºç¡€å¸¸è§é—®é¢˜è§£ç­” / Zero-Basics FAQ

### é—®é¢˜1ï¼šä¸ºä»€ä¹ˆæ¦‚ç‡å€¼è¿™ä¹ˆå°ï¼Ÿ/ Q1: Why Are Probability Values So Small?

**å›ç­” / Answer:**
- **è¿™æ˜¯æ­£å¸¸çš„ï¼** å› ä¸ºï¼š
- **This is normal!** Because:
  1. æ¦‚ç‡å€¼æœ¬èº«å°±åœ¨0åˆ°1ä¹‹é—´
   1. Probability values are between 0 and 1
  2. å¤šä¸ªæ¦‚ç‡ç›¸ä¹˜ï¼Œç»“æœä¼šè¶Šæ¥è¶Šå°
   2. Multiplying probabilities makes result smaller
  3. æ ·æœ¬è¶Šå¤šï¼Œä¹˜ç§¯è¶Šå°
   3. More samples, smaller product

**ä¾‹å­ / Example:**
- æŠ›ç¡¬å¸3æ¬¡éƒ½æ˜¯æ­£é¢ï¼š0.5Â³ = 0.125
- Flip coin 3 times all heads: 0.5Â³ = 0.125
- æŠ›ç¡¬å¸10æ¬¡éƒ½æ˜¯æ­£é¢ï¼š0.5Â¹â° â‰ˆ 0.001ï¼ˆéå¸¸å°ï¼ï¼‰
- Flip coin 10 times all heads: 0.5Â¹â° â‰ˆ 0.001 (very small!)

**é‡è¦ç†è§£ / Important Understanding:**
- æ¦‚ç‡å° â‰  ä¸å¯èƒ½
- Small probability â‰  impossible
- æ¦‚ç‡å° = è¿™ä¸ªç‰¹å®šç»„åˆå¾ˆå°‘è§ï¼Œä½†ç¡®å®å¯èƒ½å‘ç”Ÿ
- Small probability = This specific combination is rare, but can happen

### é—®é¢˜2ï¼šä»€ä¹ˆæ˜¯"æ¡ä»¶æ¦‚ç‡"ï¼Ÿ/ Q2: What is "Conditional Probability"?

**é€šä¿—è§£é‡Š / Intuitive Explanation:**
- **æ¡ä»¶æ¦‚ç‡** = åœ¨æŸä¸ª"æ¡ä»¶"ä¸‹ï¼Œäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡
- **Conditional probability** = Probability of event happening under some "condition"

**ç”Ÿæ´»ä¾‹å­ / Life Example:**
- P(ä¸‹é›¨ | é˜´å¤©) = åœ¨"é˜´å¤©"è¿™ä¸ªæ¡ä»¶ä¸‹ï¼Œä¸‹é›¨çš„æ¦‚ç‡
- P(rain | cloudy) = Probability of rain under "cloudy" condition
- é€šå¸¸æ¯”P(ä¸‹é›¨)å¤§ï¼Œå› ä¸ºé˜´å¤©æ›´å®¹æ˜“ä¸‹é›¨
- Usually larger than P(rain), because cloudy weather makes rain more likely

**åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ / In Our Example:**
- P(xâ‚=1 | y=1) = åœ¨"ç±»åˆ«æ˜¯1"çš„æ¡ä»¶ä¸‹ï¼Œç‰¹å¾1=1çš„æ¦‚ç‡
- P(xâ‚=1 | y=1) = Probability of feature 1=1 under condition "class is 1"
- è¿™å‘Šè¯‰æˆ‘ä»¬ï¼šåœ¨ç±»åˆ«1ä¸­ï¼Œç‰¹å¾1=1æœ‰å¤šå¸¸è§
- This tells us: How common is feature 1=1 in class 1

### é—®é¢˜3ï¼šè¿™äº›å‚æ•°æ˜¯æ€ä¹ˆæ¥çš„ï¼Ÿ/ Q3: How Do We Get These Parameters?

**ç®€å•æ–¹æ³•ï¼ˆé¢‘ç‡ä¼°è®¡ï¼‰/ Simple Method (Frequency Estimation):**

**æ­¥éª¤1ï¼šä¼°è®¡ç±»åˆ«å…ˆéªŒæ¦‚ç‡ / Step 1: Estimate Class Prior Probabilities**
- æ•°ä¸€æ•°è®­ç»ƒæ•°æ®ä¸­å„ç±»åˆ«æœ‰å¤šå°‘ä¸ªæ ·æœ¬
- Count how many samples of each class in training data
- P(y=1) = (ç±»åˆ«1çš„æ ·æœ¬æ•°) / (æ€»æ ·æœ¬æ•°)
- P(y=1) = (number of class 1 samples) / (total samples)

**ä¾‹å­ / Example:**
- å¦‚æœæœ‰100ä¸ªæ ·æœ¬ï¼Œ60ä¸ªæ˜¯ç±»åˆ«1ï¼Œ40ä¸ªæ˜¯ç±»åˆ«0
- If 100 samples, 60 are class 1, 40 are class 0
- é‚£ä¹ˆ P(y=1) = 60/100 = 0.6, P(y=0) = 40/100 = 0.4
- Then P(y=1) = 60/100 = 0.6, P(y=0) = 40/100 = 0.4

**æ­¥éª¤2ï¼šä¼°è®¡ç‰¹å¾æ¡ä»¶æ¦‚ç‡ / Step 2: Estimate Feature Conditional Probabilities**
- å¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œæ•°ä¸€æ•°æ¯ä¸ªç‰¹å¾å€¼å‡ºç°çš„æ¬¡æ•°
- For each class, count occurrences of each feature value
- P(xâ‚=1 | y=1) = (ç±»åˆ«1ä¸­ç‰¹å¾1=1çš„æ ·æœ¬æ•°) / (ç±»åˆ«1çš„æ€»æ ·æœ¬æ•°)
- P(xâ‚=1 | y=1) = (number of class 1 samples with feature 1=1) / (total class 1 samples)

**ä¾‹å­ / Example:**
- åœ¨60ä¸ªç±»åˆ«1çš„æ ·æœ¬ä¸­ï¼Œ30ä¸ªçš„ç‰¹å¾1=1
- Among 60 class 1 samples, 30 have feature 1=1
- é‚£ä¹ˆ P(xâ‚=1 | y=1) = 30/60 = 0.5
- Then P(xâ‚=1 | y=1) = 30/60 = 0.5

### é—®é¢˜4ï¼šä¸ºä»€ä¹ˆè¦ç”¨"ä¹˜ç§¯"è€Œä¸æ˜¯"æ±‚å’Œ"ï¼Ÿ/ Q4: Why Multiply Instead of Sum?

**å…³é”®ç†è§£ / Key Understanding:**
- å› ä¸ºæ ·æœ¬æ˜¯**ç‹¬ç«‹äº‹ä»¶**
- Because samples are **independent events**
- ç‹¬ç«‹äº‹ä»¶çš„è”åˆæ¦‚ç‡ = ä¹˜ç§¯
- Joint probability of independent events = product

**å¯¹æ¯” / Comparison:**

| æƒ…å†µ | å…¬å¼ | ä¾‹å­ |
| Situation | Formula | Example |
|--------|------|------|
| ç‹¬ç«‹äº‹ä»¶ï¼ˆæŠ›ç¡¬å¸ï¼‰ | ä¹˜ç§¯ | P(3æ¬¡æ­£é¢) = 0.5 Ã— 0.5 Ã— 0.5 |
| Independent events (coin flip) | Product | P(3 heads) = 0.5 Ã— 0.5 Ã— 0.5 |
| äº’æ–¥äº‹ä»¶ï¼ˆè¦ä¹ˆAè¦ä¹ˆBï¼‰ | æ±‚å’Œ | P(Aæˆ–B) = P(A) + P(B) |
| Mutually exclusive (either A or B) | Sum | P(A or B) = P(A) + P(B) |

**åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ / In Our Example:**
- æ ·æœ¬1å‡ºç°ä¸å½±å“æ ·æœ¬2å‡ºç°ï¼ˆç‹¬ç«‹ï¼‰
- Sample 1 appearing doesn't affect sample 2 appearing (independent)
- æ‰€ä»¥ç”¨ä¹˜ç§¯ï¼šP(æ ·æœ¬1) Ã— P(æ ·æœ¬2) Ã— P(æ ·æœ¬3)
- So use product: P(sample 1) Ã— P(sample 2) Ã— P(sample 3)

### é—®é¢˜5ï¼šä¼¼ç„¶å€¼å°ï¼Œè¯´æ˜å‚æ•°ä¸å¥½å—ï¼Ÿ/ Q5: Small Likelihood Means Bad Parameters?

**ä¸ä¸€å®šï¼/ Not necessarily!**

**é‡è¦ç†è§£ / Important Understanding:**
- ä¼¼ç„¶å€¼çš„**ç»å¯¹å€¼**ä¸é‡è¦
- **Absolute value** of likelihood doesn't matter
- é‡è¦çš„æ˜¯**ç›¸å¯¹å¤§å°**ï¼ˆä¸å…¶ä»–å‚æ•°æ¯”è¾ƒï¼‰
- What matters is **relative size** (compared to other parameters)

**ä¾‹å­ / Example:**
- Î¸â‚çš„ä¼¼ç„¶å€¼ï¼š0.003024
- Likelihood of Î¸â‚: 0.003024
- Î¸â‚‚çš„ä¼¼ç„¶å€¼ï¼š0.001000
- Likelihood of Î¸â‚‚: 0.001000
- è™½ç„¶éƒ½å¾ˆå°ï¼Œä½†Î¸â‚æ¯”Î¸â‚‚å¥½ï¼ˆå› ä¸º0.003024 > 0.001000ï¼‰
- Although both are small, Î¸â‚ is better than Î¸â‚‚ (because 0.003024 > 0.001000)

**ç±»æ¯” / Analogy:**
- å°±åƒè€ƒè¯•åˆ†æ•°ï¼š60åˆ†å’Œ80åˆ†éƒ½ä¸å®Œç¾ï¼Œä½†80åˆ†æ›´å¥½
- Like exam scores: 60 and 80 are both imperfect, but 80 is better
- æˆ‘ä»¬æ‰¾çš„æ˜¯"ç›¸å¯¹æœ€å¥½"çš„å‚æ•°ï¼Œä¸æ˜¯"ç»å¯¹å®Œç¾"çš„å‚æ•°
- We seek "relatively best" parameters, not "absolutely perfect" ones

### é—®é¢˜6ï¼šå¦‚ä½•ç†è§£"æœ€å¤§ä¼¼ç„¶ä¼°è®¡"ï¼Ÿ/ Q6: How to Understand "Maximum Likelihood Estimation"?

**é€šä¿—è§£é‡Š / Intuitive Explanation:**
- å°±åƒ"çŒœè°œæ¸¸æˆ"ï¼šä½ çœ‹åˆ°ä¸€äº›çº¿ç´¢ï¼ˆè®­ç»ƒæ•°æ®ï¼‰ï¼Œè¦çŒœå‡º"æœ€å¯èƒ½"çš„ç­”æ¡ˆï¼ˆå‚æ•°ï¼‰
- Like a "guessing game": You see some clues (training data), guess "most likely" answer (parameters)

**æ­¥éª¤ / Steps:**
1. å°è¯•ä¸€ç»„å‚æ•°Î¸â‚ï¼Œè®¡ç®—L(Î¸â‚)
1. Try parameter set Î¸â‚, calculate L(Î¸â‚)
2. å°è¯•å¦ä¸€ç»„å‚æ•°Î¸â‚‚ï¼Œè®¡ç®—L(Î¸â‚‚)
2. Try another parameter set Î¸â‚‚, calculate L(Î¸â‚‚)
3. æ¯”è¾ƒï¼šå¦‚æœL(Î¸â‚‚) > L(Î¸â‚)ï¼Œè¯´æ˜Î¸â‚‚æ›´å¥½
3. Compare: If L(Î¸â‚‚) > L(Î¸â‚), then Î¸â‚‚ is better
4. ç»§ç»­å°è¯•ï¼Œæ‰¾åˆ°ä½¿L(Î¸)æœ€å¤§çš„Î¸
4. Keep trying, find Î¸ that maximizes L(Î¸)

**å®é™…æ–¹æ³• / Practical Method:**
- é€šå¸¸ç”¨æ•°å­¦æ–¹æ³•ï¼ˆæ±‚å¯¼ã€æ¢¯åº¦ä¸‹é™ç­‰ï¼‰è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜å‚æ•°
- Usually use mathematical methods (derivatives, gradient descent, etc.) to automatically find optimal parameters
- ä¸éœ€è¦æ‰‹åŠ¨å°è¯•æ‰€æœ‰å¯èƒ½
- Don't need to manually try all possibilities

### é—®é¢˜7ï¼šä¸ºä»€ä¹ˆè¦ç”¨å¯¹æ•°ï¼Ÿ/ Q7: Why Use Logarithm?

**ç®€å•å›ç­” / Simple Answer:**
- å› ä¸ºæ¦‚ç‡ç›¸ä¹˜ä¼šå˜å¾—å¾ˆå°ï¼Œè®¡ç®—æœºå¯èƒ½æ— æ³•ç²¾ç¡®è¡¨ç¤ºï¼ˆä¸‹æº¢ï¼‰
- Because multiplying probabilities makes them very small, computer may not represent them accurately (underflow)
- å¯¹æ•°æŠŠ"ä¹˜æ³•"å˜æˆ"åŠ æ³•"ï¼Œæ›´ç¨³å®š
- Logarithm converts "multiplication" to "addition", more stable

**ä¾‹å­ / Example:**
- ç›´æ¥è®¡ç®—ï¼š0.001 Ã— 0.001 Ã— 0.001 = 0.000000001ï¼ˆå¯èƒ½ä¸¢å¤±ç²¾åº¦ï¼‰
- Direct calculation: 0.001 Ã— 0.001 Ã— 0.001 = 0.000000001 (may lose precision)
- å¯¹æ•°è®¡ç®—ï¼šlog(0.001) + log(0.001) + log(0.001) = -6.908 - 6.908 - 6.908 = -20.724ï¼ˆç¨³å®šï¼‰
- Log calculation: log(0.001) + log(0.001) + log(0.001) = -6.908 - 6.908 - 6.908 = -20.724 (stable)

### é—®é¢˜8ï¼šå¦‚ä½•éªŒè¯æˆ‘çš„è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼Ÿ/ Q8: How to Verify My Calculation?

**éªŒè¯æ–¹æ³• / Verification Methods:**

1. **æ£€æŸ¥æ¦‚ç‡å€¼èŒƒå›´ / Check Probability Range**
   - æ‰€æœ‰æ¦‚ç‡åº”è¯¥åœ¨0åˆ°1ä¹‹é—´
   - All probabilities should be between 0 and 1
   - å¦‚æœæŸä¸ªæ¦‚ç‡>1æˆ–<0ï¼Œè®¡ç®—è‚¯å®šé”™äº†
   - If any probability >1 or <0, calculation is definitely wrong

2. **æ£€æŸ¥æ¦‚ç‡å’Œ / Check Probability Sum**
   - å¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œæ‰€æœ‰å¯èƒ½å€¼çš„æ¦‚ç‡å’Œåº”è¯¥=1
   - For each feature, sum of probabilities of all possible values should = 1
   - æ¯”å¦‚ï¼šP(xâ‚=1|y=1) + P(xâ‚=2|y=1) = 0.5 + 0.5 = 1.0 âœ“
   - E.g.: P(xâ‚=1|y=1) + P(xâ‚=2|y=1) = 0.5 + 0.5 = 1.0 âœ“

3. **éªŒè¯å¯¹æ•°å…³ç³» / Verify Logarithm Relationship**
   - log(L(Î¸)) åº”è¯¥ç­‰äº â„“(Î¸)
   - log(L(Î¸)) should equal â„“(Î¸)
   - æ¯”å¦‚ï¼šlog(0.003024) â‰ˆ -5.800 = â„“(Î¸) âœ“
   - E.g.: log(0.003024) â‰ˆ -5.800 = â„“(Î¸) âœ“

---

### 2.3 æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ / Laplacian Distribution

**ä¸­æ–‡è§£é‡Šï¼š**
æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒç”¨äºå»ºæ¨¡å®å€¼ç‰¹å¾ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š

**English Explanation:**
The Laplacian distribution is used to model real-valued features, with probability density function:

**æ¦‚ç‡å¯†åº¦å‡½æ•° / PDF:**  
$$
p(x\mid \mu,\sigma) = \frac{1}{2\sigma}\exp\!\left(-\frac{|x-\mu|}{\sigma}\right)
$$

**ç¬¦å·è¯´æ˜ / Symbol Explanation:**
- $p(x\mid \mu,\sigma)$ï¼šåœ¨å‚æ•°$\mu$å’Œ$\sigma$ä¸‹ï¼Œéšæœºå˜é‡å–å€¼ä¸ºxçš„æ¦‚ç‡å¯†åº¦ / Probability density of random variable taking value x given parameters $\mu$ and $\sigma$
- $x$ï¼šéšæœºå˜é‡çš„å–å€¼ï¼ˆå®æ•°ï¼‰/ Value of random variable (real number)
- $\mu$ï¼šä½ç½®å‚æ•°ï¼Œä¹Ÿæ˜¯åˆ†å¸ƒçš„ä¸­ä½æ•°å’Œä¼—æ•° / Location parameter, also median and mode of distribution
- $\sigma$ï¼šå°ºåº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å®½åº¦ï¼ˆå¿…é¡»>0ï¼‰/ Scale parameter, controls width of distribution (must be >0)
- $|x-\mu|$ï¼šxä¸$\mu$çš„ç»å¯¹è·ç¦» / Absolute distance between x and $\mu$
- $\exp$ï¼šè‡ªç„¶æŒ‡æ•°å‡½æ•°ï¼Œ$e$çš„å¹‚æ¬¡ / Natural exponential function, $e$ raised to power
- $\frac{1}{2\sigma}$ï¼šå½’ä¸€åŒ–ç³»æ•°ï¼Œç¡®ä¿æ¦‚ç‡å¯†åº¦å‡½æ•°ç§¯åˆ†ä¸º1 / Normalization coefficient, ensures PDF integrates to 1

**è®¡ç®—æ­¥éª¤ / Calculation Steps:**
1. è®¡ç®— $|x-\mu|$ï¼ˆxä¸$\mu$çš„ç»å¯¹è·ç¦»ï¼‰/ Calculate $|x-\mu|$ (absolute distance)
2. è®¡ç®— $\frac{|x-\mu|}{\sigma}$ï¼ˆå½’ä¸€åŒ–è·ç¦»ï¼‰/ Calculate $\frac{|x-\mu|}{\sigma}$ (normalized distance)
3. è®¡ç®— $-\frac{|x-\mu|}{\sigma}$ï¼ˆå–è´Ÿå·ï¼‰/ Calculate $-\frac{|x-\mu|}{\sigma}$ (take negative)
4. è®¡ç®— $\exp(-\frac{|x-\mu|}{\sigma})$ï¼ˆeçš„è´Ÿå½’ä¸€åŒ–è·ç¦»æ¬¡æ–¹ï¼‰/ Calculate $\exp(-\frac{|x-\mu|}{\sigma})$ (e to power of negative normalized distance)
5. ä¹˜ä»¥å½’ä¸€åŒ–ç³»æ•°$\frac{1}{2\sigma}$ / Multiply by normalization coefficient $\frac{1}{2\sigma}$

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
å‡è®¾å‚æ•°Î¼=2, Ïƒ=1ï¼Œè®¡ç®—x=3å¤„çš„æ¦‚ç‡å¯†åº¦
Suppose parameters Î¼=2, Ïƒ=1, calculate probability density at x=3

æ­¥éª¤1: |3-2| = 1
æ­¥éª¤2: 1/1 = 1
æ­¥éª¤3: -1
æ­¥éª¤4: exp(-1) â‰ˆ 0.368
æ­¥éª¤5: (1/(2Ã—1)) Ã— 0.368 = 0.5 Ã— 0.368 = 0.184

æ‰€ä»¥ p(3|Î¼=2,Ïƒ=1) â‰ˆ 0.184
So p(3|Î¼=2,Ïƒ=1) â‰ˆ 0.184

å†è®¡ç®—x=2ï¼ˆåœ¨Î¼å¤„ï¼‰:
Step 1: |2-2| = 0
Step 2: 0/1 = 0
Step 3: -0 = 0
Step 4: exp(0) = 1
Step 5: 0.5 Ã— 1 = 0.5

åœ¨Î¼å¤„æ¦‚ç‡å¯†åº¦æœ€å¤§ï¼ˆå³°å€¼ï¼‰
Probability density is maximum at Î¼ (peak)

**ç‰¹ç‚¹ / Characteristics:**
- åˆ†å¸ƒå…³äº$\mu$å¯¹ç§° / Distribution is symmetric about $\mu$
- åœ¨$\mu$å¤„è¾¾åˆ°å³°å€¼ / Peak at $\mu$
- å°¾éƒ¨æ¯”é«˜æ–¯åˆ†å¸ƒæ›´åšï¼ˆé‡å°¾åˆ†å¸ƒï¼‰/ Heavier tails than Gaussian distribution

### 2.4 å‚æ•°ä¼°è®¡ / Parameter Estimation

**å¯¹äºæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ / For Laplacian Distribution:**

**ä½ç½®å‚æ•° Î¼ / Location Parameter Î¼:**
- æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ä¸­ä½æ•° / MLE is the median
- å› ä¸ºæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„ä¸­ä½æ•°ç­‰äºä½ç½®å‚æ•° / Because the median of Laplacian equals the location parameter

**å°ºåº¦å‚æ•° Ïƒ / Scale Parameter Ïƒ:**
é€šè¿‡å¯¹æ•°ä¼¼ç„¶å¯¹ Ïƒ æ±‚å¯¼å¹¶ä»¤å…¶ä¸ºé›¶å¾—åˆ° / Obtained by taking derivative of log-likelihood w.r.t. Ïƒ and setting to zero

---

## çº¿æ€§åˆ¤åˆ«åˆ†æ / Linear Discriminant Analysis

### 3.1 åŸºæœ¬æ¦‚å¿µ / Basic Concepts

**ä¸­æ–‡è§£é‡Šï¼š**
çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰å‡è®¾æ¯ä¸ªç±»åˆ«çš„æ•°æ®æœä»é«˜æ–¯åˆ†å¸ƒï¼Œé€šè¿‡è´å¶æ–¯å®šç†è®¡ç®—åéªŒæ¦‚ç‡ã€‚

**English Explanation:**
Linear Discriminant Analysis (LDA) assumes data from each class follows a Gaussian distribution and computes posterior probabilities using Bayes' theorem.

### 3.2 åéªŒæ¦‚ç‡çš„Sigmoidå½¢å¼ / Sigmoid Form of Posterior

**ä¸­æ–‡è§£é‡Šï¼š**
åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼ŒLDAçš„åéªŒæ¦‚ç‡å¯ä»¥å†™æˆsigmoidå‡½æ•°çš„å½¢å¼ã€‚

**English Explanation:**
In binary classification, the posterior probability of LDA can be written in sigmoid form.

**æ¨å¯¼è¿‡ç¨‹ / Derivation:**

1. ä½¿ç”¨è´å¶æ–¯å®šç† / Using Bayes' theorem:  
$$
p(y=1\mid x) = \frac{p(x\mid y=1)p(y=1)}{p(x\mid y=1)p(y=1) + p(x\mid y=0)p(y=0)}
$$

2. å‡è®¾é«˜æ–¯åˆ†å¸ƒ / Assuming Gaussian distribution:  
$$
p(x\mid y=c) = \mathcal{N}(x;\,\mu_c,\Sigma)
$$

3. ç»è¿‡ä»£æ•°å˜æ¢å¾—åˆ° / After algebraic manipulation:  
$$
p(y=1\mid x) = \frac{1}{1+\exp(-\theta_0 - \theta^\top x)}
$$

å…¶ä¸­ / where:  
$$
\theta_0 = \log\frac{p(y=1)}{p(y=0)} - \frac{1}{2}\big(\mu_1^\top\Sigma^{-1}\mu_1 - \mu_0^\top\Sigma^{-1}\mu_0\big),\quad
\theta = \Sigma^{-1}(\mu_1 - \mu_0)
$$

### 3.3 ä¸é€»è¾‘å›å½’çš„å…³ç³» / Relationship with Logistic Regression

**ä¸­æ–‡è§£é‡Šï¼š**
LDAå’Œé€»è¾‘å›å½’éƒ½äº§ç”Ÿsigmoidå½¢å¼çš„åˆ†ç±»å™¨ï¼Œä½†å‡è®¾ä¸åŒï¼š
- LDAå‡è®¾é«˜æ–¯åˆ†å¸ƒå’Œå…±äº«åæ–¹å·®çŸ©é˜µ
- é€»è¾‘å›å½’ä¸åšåˆ†å¸ƒå‡è®¾

**English Explanation:**
Both LDA and logistic regression produce sigmoid classifiers, but with different assumptions:
- LDA assumes Gaussian distribution and shared covariance matrix
- Logistic regression makes no distributional assumptions

---

## KLæ•£åº¦ / KL Divergence

### 4.1 å®šä¹‰ / Definition

**ä¸­æ–‡è§£é‡Šï¼š**
KLæ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚

**English Explanation:**
KL Divergence measures the difference between two probability distributions.

**å®šä¹‰ / Definition:**  
$$
D_{\mathrm{KL}}(P\|Q) = \int P(x)\,\log\frac{P(x)}{Q(x)}\,dx = \mathbb{E}_{P}\big[\log P(x)-\log Q(x)\big]
$$

**ç¬¦å·è¯´æ˜ / Symbol Explanation:**
- $D_{\mathrm{KL}}(P\|Q)$ï¼šä»åˆ†å¸ƒQåˆ°åˆ†å¸ƒPçš„KLæ•£åº¦ï¼ˆæ³¨æ„é¡ºåºï¼ï¼‰/ KL divergence from distribution Q to distribution P (note the order!)
- $P(x)$ï¼šåˆ†å¸ƒPåœ¨xå¤„çš„æ¦‚ç‡å¯†åº¦ / Probability density of distribution P at x
- $Q(x)$ï¼šåˆ†å¸ƒQåœ¨xå¤„çš„æ¦‚ç‡å¯†åº¦ / Probability density of distribution Q at x
- $\int$ï¼šç§¯åˆ†ç¬¦å·ï¼Œå¯¹æ‰€æœ‰å¯èƒ½çš„xå€¼ç§¯åˆ† / Integral symbol, integrate over all possible values of x
- $\log$ï¼šè‡ªç„¶å¯¹æ•°ï¼ˆæˆ–å¸¸ç”¨å¯¹æ•°ï¼‰/ Natural logarithm (or common logarithm)
- $\frac{P(x)}{Q(x)}$ï¼šä¸¤ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦æ¯”å€¼ / Ratio of probability densities of two distributions
- $\mathbb{E}_{P}[\cdot]$ï¼šåœ¨åˆ†å¸ƒPä¸‹çš„æœŸæœ›å€¼ / Expectation under distribution P
- $dx$ï¼šå¯¹xçš„ç§¯åˆ† / Integration with respect to x

**è®¡ç®—æ­¥éª¤ / Calculation Steps:**
1. å¯¹æ¯ä¸ªxå€¼ï¼Œè®¡ç®— $\frac{P(x)}{Q(x)}$ï¼ˆä¸¤ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦æ¯”ï¼‰/ For each x, calculate $\frac{P(x)}{Q(x)}$ (ratio of probability densities)
2. è®¡ç®— $\log\frac{P(x)}{Q(x)} = \log P(x) - \log Q(x)$ / Calculate $\log\frac{P(x)}{Q(x)} = \log P(x) - \log Q(x)$
3. ä¹˜ä»¥$P(x)$å¾—åˆ° $P(x) \log\frac{P(x)}{Q(x)}$ / Multiply by $P(x)$ to get $P(x) \log\frac{P(x)}{Q(x)}$
4. å¯¹æ‰€æœ‰xç§¯åˆ†ï¼ˆæˆ–æ±‚å’Œï¼Œå¦‚æœæ˜¯ç¦»æ•£åˆ†å¸ƒï¼‰/ Integrate (or sum, if discrete) over all x
5. ç»“æœæ€»æ˜¯â‰¥0ï¼Œå½“ä¸”ä»…å½“P=Qæ—¶ç­‰äº0 / Result is always â‰¥0, equals 0 if and only if P=Q

**è®¡ç®—ç¤ºä¾‹ / Calculation Example:**
å‡è®¾ä¸¤ä¸ªç¦»æ•£åˆ†å¸ƒï¼š
Suppose two discrete distributions:

P: P(0)=0.5, P(1)=0.5
Q: Q(0)=0.8, Q(1)=0.2

è®¡ç®—KLæ•£åº¦ D_KL(P||Q) / Calculate KL divergence D_KL(P||Q):
- å½“x=0: P(0)Ã—log(P(0)/Q(0)) = 0.5Ã—log(0.5/0.8) = 0.5Ã—log(0.625) = 0.5Ã—(-0.470) = -0.235
- å½“x=1: P(1)Ã—log(P(1)/Q(1)) = 0.5Ã—log(0.5/0.2) = 0.5Ã—log(2.5) = 0.5Ã—0.916 = 0.458
- D_KL(P||Q) = -0.235 + 0.458 = 0.223

æ³¨æ„ï¼šD_KL(Q||P)ä¼šå¾—åˆ°ä¸åŒçš„å€¼ï¼ˆéå¯¹ç§°æ€§ï¼‰
Note: D_KL(Q||P) would give a different value (asymmetry)

**é‡è¦æ€§è´¨ / Important Properties:**
- éå¯¹ç§°ï¼š$D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P)$ é€šå¸¸ / Asymmetric: usually $D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P)$
- éè´Ÿï¼š$D_{\mathrm{KL}}(P\|Q) \geq 0$ / Non-negative: $D_{\mathrm{KL}}(P\|Q) \geq 0$
- å½“P=Qæ—¶ï¼ŒKLæ•£åº¦ä¸º0 / When P=Q, KL divergence is 0

### 4.2 æ€§è´¨ / Properties

1. **éå¯¹ç§°æ€§ / Asymmetry:**
   - D_KL(P||Q) â‰  D_KL(Q||P) é€šå¸¸ / in general

2. **éè´Ÿæ€§ / Non-negativity:**
   - D_KL(P||Q) â‰¥ 0
   - å½“ä¸”ä»…å½“ P = Q æ—¶ç­‰äº0 / Equals 0 if and only if P = Q

3. **ä¸æ˜¯çœŸæ­£çš„è·ç¦» / Not a True Distance:**
   - ä¸æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼ / Does not satisfy triangle inequality

### 4.3 å¯¹ç§°KLæ•£åº¦ï¼ˆJeffreysæ•£åº¦ï¼‰/ Symmetrized KL (Jeffreys Divergence)

**å®šä¹‰ / Definition:**  
$$
J(P_1, P_2) = D_{\mathrm{KL}}(P_1\|P_2) + D_{\mathrm{KL}}(P_2\|P_1)
$$

**ä¸­æ–‡è§£é‡Šï¼š**
å¯¹ç§°KLæ•£åº¦é€šè¿‡å°†ä¸¤ä¸ªæ–¹å‘çš„KLæ•£åº¦ç›¸åŠ ï¼Œå¾—åˆ°ä¸€ä¸ªå¯¹ç§°çš„åº¦é‡ã€‚

**English Explanation:**
Symmetrized KL divergence adds KL divergences in both directions to get a symmetric measure.

### 4.4 é«˜æ–¯åˆ†å¸ƒçš„KLæ•£åº¦ / KL Divergence for Gaussian Distributions

**å¯¹äºå¤šå…ƒé«˜æ–¯åˆ†å¸ƒ / For Multivariate Gaussian:**

ä¸¤ä¸ªNç»´é«˜æ–¯åˆ†å¸ƒ Pâ‚ = N(Î¼â‚, Î£â‚) å’Œ Pâ‚‚ = N(Î¼â‚‚, Î£â‚‚) çš„å¯¹ç§°KLæ•£åº¦ä¸ºï¼š
For two N-dimensional Gaussians Pâ‚ = N(Î¼â‚, Î£â‚) and Pâ‚‚ = N(Î¼â‚‚, Î£â‚‚), the symmetrized KL divergence is:

$$
J(P_1, P_2) = \frac{1}{2}\text{tr}\big(\Sigma_1^{-1}\Sigma_2 + \Sigma_2^{-1}\Sigma_1 - 2I\big)
          + \frac{1}{2}(\mu_1-\mu_2)^\top(\Sigma_1^{-1}+\Sigma_2^{-1})(\mu_1-\mu_2)
$$

**å…³é”®æŠ€å·§ / Key Techniques:**
- ä½¿ç”¨è¿¹çš„å¾ªç¯æ€§è´¨ / Using cyclic property of trace
- tr(AB) = tr(BA)
- tr((x-Î¼)áµ€Î£â»Â¹(x-Î¼)) = tr(Î£â»Â¹(x-Î¼)(x-Î¼)áµ€)

---

## å­¦ä¹ å»ºè®® / Study Recommendations

### å¯¹äºåˆå­¦è€… / For Beginners:

1. **å…ˆæŒæ¡åŸºç¡€æ¦‚ç‡è®º / Master Basic Probability First:**
   - æ¡ä»¶æ¦‚ç‡ / Conditional probability
   - è´å¶æ–¯å®šç† / Bayes' theorem
   - æœŸæœ›å’Œæ–¹å·® / Expectation and variance

2. **ç†è§£ä¿¡æ¯è®ºæ¦‚å¿µ / Understand Information Theory:**
   - ä»ç†µçš„ç›´è§‚ç†è§£å¼€å§‹ / Start with intuitive understanding of entropy
   - ä¿¡æ¯é‡ = ä¸ç¡®å®šæ€§ / Information = Uncertainty

3. **ç»ƒä¹ æ¨å¯¼ / Practice Derivations:**
   - ä¸è¦åªçœ‹ç­”æ¡ˆ / Don't just read answers
   - è‡ªå·±æ¨å¯¼ä¸€é / Derive yourself
   - ç†è§£æ¯ä¸€æ­¥çš„æ•°å­¦åŸç† / Understand the math behind each step

4. **ç¼–ç¨‹å®ç° / Programming Implementation:**
   - å®ç°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ / Implement Naive Bayes classifier
   - è®¡ç®—ç†µå’Œäº’ä¿¡æ¯ / Compute entropy and mutual information
   - å¯è§†åŒ–é«˜æ–¯åˆ†å¸ƒ / Visualize Gaussian distributions

### å¸¸è§é”™è¯¯ / Common Mistakes:

1. **æ··æ·†è”åˆæ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡ / Confusing joint and conditional probability**
2. **å¿˜è®°å¯¹æ•°ä¼¼ç„¶çš„è´Ÿå· / Forgetting negative sign in log-likelihood**
3. **ä¸ç†è§£æŒ‡ç¤ºå‡½æ•°çš„æœŸæœ› / Not understanding expectation of indicator function**
4. **KLæ•£åº¦çš„æ–¹å‘æ··æ·† / Confusing direction of KL divergence**

---

## ç»ƒä¹ é¢˜ / Practice Problems

### é—®é¢˜1 / Problem 1:
è®¡ç®—ä¸€ä¸ªä¸å…¬å¹³ç¡¬å¸çš„ç†µï¼Œå…¶ä¸­P(æ­£é¢) = 0.7
Calculate the entropy of an unfair coin with P(heads) = 0.7

### é—®é¢˜2 / Problem 2:
è¯æ˜å¦‚æœXå’ŒYç‹¬ç«‹ï¼Œåˆ™H(X, Y) = H(X) + H(Y)
Prove that if X and Y are independent, then H(X, Y) = H(X) + H(Y)

### é—®é¢˜3 / Problem 3:
æ¨å¯¼æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡
Derive the maximum likelihood estimates for Laplacian distribution

---

## å‚è€ƒèµ„æº / Reference Resources

1. **æ¦‚ç‡è®º / Probability:**
   - ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹/ "Probability and Mathematical Statistics"
   - Introduction to Probability (Blitzstein & Hwang)

2. **ä¿¡æ¯è®º / Information Theory:**
   - Elements of Information Theory (Cover & Thomas)

3. **æœºå™¨å­¦ä¹  / Machine Learning:**
   - Pattern Recognition and Machine Learning (Bishop)
   - Machine Learning: A Probabilistic Perspective (Murphy)

---

## ä¾‹é¢˜ä¸è§£ç­” / Worked Examples

### ä¾‹é¢˜1ï¼šåç½®éª°å­å¶æ•°æ¦‚ç‡ / Biased Die Even Probability

**é¢˜ç›® / Question:**  
ä¸€ä¸ªåç½®éª°å­å…­ä¸ªé¢çš„æ¦‚ç‡åˆ†å¸ƒå¦‚ä¸‹ï¼š
A biased die has the following probabilities of landing on each face:

| é¢ / Face | 1 | 2 | 3 | 4 | 5 | 6 |
|----------|---|---|---|---|---|---|
| æ¦‚ç‡ P(face) | 0.1 | 0.1 | 0.2 | 0.2 | 0.4 | 0 |

å¦‚æœæ·å‡ºå¶æ•°å°±è·èƒœï¼Œæ±‚è·èƒœçš„æ¦‚ç‡ã€‚è¿™ä¸ªæ¦‚ç‡æ¯”å…¬å¹³éª°å­ï¼ˆæ¯ä¸ªé¢æ¦‚ç‡ç›¸ç­‰ï¼‰æ›´å¥½è¿˜æ˜¯æ›´å·®ï¼Ÿ
I win if the die shows even. What is the probability that I win? Is this better or worse than a fair die (i.e., a die with equal probabilities for each face)?

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**æ­¥éª¤1ï¼šç†è§£é—®é¢˜ / Step 1: Understand the Problem**
- å¶æ•°é¢ï¼š2, 4, 6
- Even faces: 2, 4, 6
- éœ€è¦è®¡ç®—ï¼šP(2) + P(4) + P(6)
- Need to calculate: P(2) + P(4) + P(6)

**æ­¥éª¤2ï¼šè®¡ç®—æ¦‚ç‡ / Step 2: Calculate Probability**

$$P(\text{even}) = P(2) + P(4) + P(6)$$

ä»æ¦‚ç‡è¡¨ä¸­æŸ¥æ‰¾ï¼š
Look up from probability table:
- P(2) = 0.1
- P(4) = 0.2
- P(6) = 0

**æ­¥éª¤3ï¼šæ±‚å’Œ / Step 3: Sum**

$$P(\text{even}) = 0.1 + 0.2 + 0 = 0.3$$

**æ­¥éª¤4ï¼šä¸å…¬å¹³éª°å­æ¯”è¾ƒ / Step 4: Compare with Fair Die**

å…¬å¹³éª°å­æ¯ä¸ªé¢çš„æ¦‚ç‡éƒ½æ˜¯1/6ï¼š
Fair die has probability 1/6 for each face:

$$P_{\text{fair}}(\text{even}) = P(2) + P(4) + P(6) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{3}{6} = 0.5$$

**ç»“è®º / Conclusion:**
- åç½®éª°å­ï¼šP(even) = 0.3 = 30%
- Biased die: P(even) = 0.3 = 30%
- å…¬å¹³éª°å­ï¼šP(even) = 0.5 = 50%
- Fair die: P(even) = 0.5 = 50%
- **åç½®éª°å­æ›´å·®**ï¼ˆè·èƒœæ¦‚ç‡æ›´ä½ï¼‰
- **Biased die is worse** (lower winning probability)

**å…³é”®è¯ / Keywords:** æ¦‚ç‡æ±‚å’Œã€äº‹ä»¶å¹¶é›†ã€äº’æ–¥äº‹ä»¶ã€‚

### ä¾‹é¢˜2ï¼šæŒ‡ç¤ºå‡½æ•°æœŸæœ› / Expectation of Indicator

**é¢˜ç›® / Question:**  
è®¾éšæœºå˜é‡Xå¯ä»¥å–å€¼3ã€8æˆ–9ï¼Œå¯¹åº”çš„æ¦‚ç‡åˆ†åˆ«ä¸ºpâ‚ƒã€pâ‚ˆå’Œpâ‚‰ã€‚
Let X be a random variable which takes on the values 3, 8 or 9 with probabilities pâ‚ƒ, pâ‚ˆ and pâ‚‰ respectively.

è®¡ç®—æŒ‡ç¤ºå‡½æ•°çš„æœŸæœ›å€¼ï¼šE[I[X = 8]]
Calculate the expected value of the indicator function: E[I[X = 8]]

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**æ­¥éª¤1ï¼šç†è§£æŒ‡ç¤ºå‡½æ•° / Step 1: Understand Indicator Function**

æŒ‡ç¤ºå‡½æ•°çš„å®šä¹‰ï¼š
Definition of indicator function:

$$I[X = 8] = \begin{cases} 1, & \text{if } X = 8 \\ 0, & \text{otherwise} \end{cases}$$

**æ­¥éª¤2ï¼šåº”ç”¨æœŸæœ›çš„å®šä¹‰ / Step 2: Apply Definition of Expectation**

æœŸæœ›å€¼çš„å®šä¹‰ï¼š
Definition of expected value:

$$E[I[X = 8]] = \sum_{x \in \{3,8,9\}} I[X = 8] \cdot P(X = x)$$

**æ­¥éª¤3ï¼šå±•å¼€æ±‚å’Œ / Step 3: Expand Sum**

$$E[I[X = 8]] = I[3 = 8] \cdot P(X = 3) + I[8 = 8] \cdot P(X = 8) + I[9 = 8] \cdot P(X = 9)$$

**æ­¥éª¤4ï¼šè®¡ç®—æŒ‡ç¤ºå‡½æ•°çš„å€¼ / Step 4: Calculate Indicator Values**

- I[3 = 8] = 0ï¼ˆå› ä¸º3 â‰  8ï¼‰
- I[3 = 8] = 0 (because 3 â‰  8)
- I[8 = 8] = 1ï¼ˆå› ä¸º8 = 8ï¼‰
- I[8 = 8] = 1 (because 8 = 8)
- I[9 = 8] = 0ï¼ˆå› ä¸º9 â‰  8ï¼‰
- I[9 = 8] = 0 (because 9 â‰  8)

**æ­¥éª¤5ï¼šä»£å…¥è®¡ç®— / Step 5: Substitute and Calculate**

$$E[I[X = 8]] = 0 \cdot p_3 + 1 \cdot p_8 + 0 \cdot p_9 = p_8$$

**ç»“è®º / Conclusion:**
$$E[I[X = 8]] = p_8$$

**é‡è¦ç†è§£ / Important Understanding:**
- æŒ‡ç¤ºå‡½æ•°çš„æœŸæœ›å€¼ç­‰äºè¯¥äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡
- Expected value of indicator function equals probability of that event
- è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ€§è´¨ï¼Œåœ¨æ¦‚ç‡è®ºä¸­ç»å¸¸ä½¿ç”¨
- This is a very important property, frequently used in probability theory

**å…³é”®è¯ / Keywords:** æŒ‡ç¤ºå‡½æ•°ã€æœŸæœ›çº¿æ€§æ€§ã€æ¦‚ç‡ä¸æœŸæœ›çš„å…³ç³»ã€‚

### ä¾‹é¢˜3ï¼šç†µçš„é“¾å¼æ³•åˆ™ / Chain Rule of Entropy

**é¢˜ç›® / Question:**  
ä½¿ç”¨ç†µã€è”åˆç†µå’Œæ¡ä»¶ç†µçš„å®šä¹‰ï¼Œè¯æ˜ç†µçš„é“¾å¼æ³•åˆ™ï¼š
Using the definitions of entropy, joint entropy, and conditional entropy, prove the chain rule for entropy:

$$H(X, Y) = H(Y) + H(X|Y)$$

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**æ­¥éª¤1ï¼šå†™å‡ºå®šä¹‰ / Step 1: Write Down Definitions**

**è”åˆç†µçš„å®šä¹‰ / Definition of Joint Entropy:**
$$H(X, Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(X=x, Y=y) \log_2 P(X=x, Y=y)$$

**æ¡ä»¶ç†µçš„å®šä¹‰ / Definition of Conditional Entropy:**
$$H(X|Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(X=x, Y=y) \log_2 P(X=x|Y=y)$$

**ç†µçš„å®šä¹‰ / Definition of Entropy:**
$$H(Y) = -\sum_{y \in \mathcal{Y}} P(Y=y) \log_2 P(Y=y)$$

**æ­¥éª¤2ï¼šä½¿ç”¨æ¡ä»¶æ¦‚ç‡å…¬å¼ / Step 2: Use Conditional Probability Formula**

æ ¹æ®æ¡ä»¶æ¦‚ç‡çš„å®šä¹‰ï¼š
According to definition of conditional probability:

$$P(X=x, Y=y) = P(Y=y) \cdot P(X=x|Y=y)$$

**æ­¥éª¤3ï¼šå±•å¼€è”åˆç†µ / Step 3: Expand Joint Entropy**

å°†æ¡ä»¶æ¦‚ç‡å…¬å¼ä»£å…¥è”åˆç†µï¼š
Substitute conditional probability formula into joint entropy:

$$H(X, Y) = -\sum_{x,y} P(X=x, Y=y) \log_2 P(X=x, Y=y)$$

$$= -\sum_{x,y} P(Y=y) P(X=x|Y=y) \log_2 [P(Y=y) P(X=x|Y=y)]$$

**æ­¥éª¤4ï¼šä½¿ç”¨å¯¹æ•°æ€§è´¨åˆ†è§£ / Step 4: Decompose Using Logarithm Properties**

ä½¿ç”¨å¯¹æ•°çš„ä¹˜ç§¯æ€§è´¨ï¼š$\log(ab) = \log a + \log b$
Use logarithm product property: $\log(ab) = \log a + \log b$

$$= -\sum_{x,y} P(Y=y) P(X=x|Y=y) [\log_2 P(Y=y) + \log_2 P(X=x|Y=y)]$$

**æ­¥éª¤5ï¼šå±•å¼€å¹¶åˆ†ç¦»é¡¹ / Step 5: Expand and Separate Terms**

$$= -\sum_{x,y} P(Y=y) P(X=x|Y=y) \log_2 P(Y=y) - \sum_{x,y} P(Y=y) P(X=x|Y=y) \log_2 P(X=x|Y=y)$$

**æ­¥éª¤6ï¼šç®€åŒ–ç¬¬ä¸€é¡¹ / Step 6: Simplify First Term**

å¯¹xæ±‚å’Œï¼Œåˆ©ç”¨ $\sum_x P(X=x|Y=y) = 1$ï¼š
Sum over x, using $\sum_x P(X=x|Y=y) = 1$:

$$-\sum_{x,y} P(Y=y) P(X=x|Y=y) \log_2 P(Y=y) = -\sum_y P(Y=y) \log_2 P(Y=y) = H(Y)$$

**æ­¥éª¤7ï¼šç®€åŒ–ç¬¬äºŒé¡¹ / Step 7: Simplify Second Term**

ç¬¬äºŒé¡¹å°±æ˜¯æ¡ä»¶ç†µçš„å®šä¹‰ï¼š
Second term is the definition of conditional entropy:

$$-\sum_{x,y} P(Y=y) P(X=x|Y=y) \log_2 P(X=x|Y=y) = H(X|Y)$$

**æ­¥éª¤8ï¼šå¾—å‡ºç»“è®º / Step 8: Conclude**

$$H(X, Y) = H(Y) + H(X|Y) \quad \square$$

**éªŒè¯ / Verification:**
åŒæ ·å¯ä»¥è¯æ˜ï¼š$H(X, Y) = H(X) + H(Y|X)$
Similarly we can prove: $H(X, Y) = H(X) + H(Y|X)$

**å…³é”®è¯ / Keywords:** æ¡ä»¶æ¦‚ç‡ã€å¯¹æ•°åˆ†è§£ã€é“¾å¼æ³•åˆ™ã€è”åˆç†µã€‚

### ä¾‹é¢˜4ï¼šç‹¬ç«‹éšæœºå˜é‡çš„äº’ä¿¡æ¯ / Mutual Information of Independent Variables

**é¢˜ç›® / Question:**  
å›å¿†ä¸¤ä¸ªéšæœºå˜é‡Xå’ŒYç‹¬ç«‹çš„å®šä¹‰ï¼šå¯¹äºæ‰€æœ‰x âˆˆ Xå’Œæ‰€æœ‰y âˆˆ Yï¼Œæœ‰
Recall that two random variables X and Y are independent if for all x âˆˆ X and all y âˆˆ Y:

$$P(X=x, Y=y) = P(X=x) P(Y=y)$$

å¦‚æœå˜é‡Xå’ŒYç‹¬ç«‹ï¼Œé‚£ä¹ˆI(X; Y) = 0å—ï¼Ÿå¦‚æœæ˜¯ï¼Œè¯·è¯æ˜ï¼›å¦‚æœä¸æ˜¯ï¼Œè¯·ç»™å‡ºåä¾‹ã€‚
If variables X and Y are independent, is I(X; Y) = 0? If yes, prove it. If no, give a counterexample.

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**ç­”æ¡ˆï¼šæ˜¯çš„ï¼ŒI(X; Y) = 0 / Answer: Yes, I(X; Y) = 0**

**æ­¥éª¤1ï¼šå†™å‡ºäº’ä¿¡æ¯çš„å®šä¹‰ / Step 1: Write Definition of Mutual Information**

$$I(X; Y) = \sum_{x,y} P(X=x, Y=y) \log_2 \frac{P(X=x, Y=y)}{P(X=x) P(Y=y)}$$

**æ­¥éª¤2ï¼šä½¿ç”¨ç‹¬ç«‹æ€§æ¡ä»¶ / Step 2: Use Independence Condition**

ç”±äºXå’ŒYç‹¬ç«‹ï¼š
Since X and Y are independent:

$$P(X=x, Y=y) = P(X=x) P(Y=y)$$

**æ­¥éª¤3ï¼šä»£å…¥äº’ä¿¡æ¯å…¬å¼ / Step 3: Substitute into Mutual Information Formula**

$$I(X; Y) = \sum_{x,y} P(X=x) P(Y=y) \log_2 \frac{P(X=x) P(Y=y)}{P(X=x) P(Y=y)}$$

**æ­¥éª¤4ï¼šç®€åŒ– / Step 4: Simplify**

$$\frac{P(X=x) P(Y=y)}{P(X=x) P(Y=y)} = 1$$

æ‰€ä»¥ï¼š
Therefore:

$$I(X; Y) = \sum_{x,y} P(X=x) P(Y=y) \log_2 1$$

**æ­¥éª¤5ï¼šè®¡ç®—å¯¹æ•° / Step 5: Calculate Logarithm**

$$\log_2 1 = 0$$

**æ­¥éª¤6ï¼šå¾—å‡ºç»“è®º / Step 6: Conclude**

$$I(X; Y) = \sum_{x,y} P(X=x) P(Y=y) \cdot 0 = 0 \quad \square$$

**é‡è¦ç†è§£ / Important Understanding:**
- äº’ä¿¡æ¯è¡¡é‡ä¸¤ä¸ªå˜é‡çš„ç›¸äº’ä¾èµ–ç¨‹åº¦
- Mutual information measures mutual dependence between two variables
- å¦‚æœä¸¤ä¸ªå˜é‡ç‹¬ç«‹ï¼Œå®ƒä»¬ä¹‹é—´æ²¡æœ‰ä¿¡æ¯å…±äº«ï¼Œæ‰€ä»¥äº’ä¿¡æ¯ä¸º0
- If two variables are independent, they share no information, so mutual information is 0
- è¿™æ˜¯äº’ä¿¡æ¯çš„ä¸€ä¸ªé‡è¦æ€§è´¨
- This is an important property of mutual information

**å…³é”®è¯ / Keywords:** ç‹¬ç«‹æ€§ã€äº’ä¿¡æ¯ã€æ¡ä»¶æ¦‚ç‡ã€å¯¹æ•°æ€§è´¨ã€‚

---

### ä¾‹é¢˜5ï¼šæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ / MLE for Laplacian Distribution

**é¢˜ç›® / Question:**  
ç»™å®šè®­ç»ƒé›† $D = \{(x^{(i)}, y^{(i)}); i = 1, \ldots, M\}$ï¼Œå…¶ä¸­ $x^{(i)} \in \mathbb{R}^N$ ä¸” $y^{(i)} \in \{1, 2, \ldots, C\}$ï¼Œæ¨å¯¼æœ´ç´ è´å¶æ–¯å¯¹å®å€¼ $x_j^{(i)}$ ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒå»ºæ¨¡æ—¶çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚
Given a training set $D = \{(x^{(i)}, y^{(i)}); i = 1, \ldots, M\}$, where $x^{(i)} \in \mathbb{R}^N$ and $y^{(i)} \in \{1, 2, \ldots, C\}$, derive the maximum likelihood estimates of naive Bayes for real valued $x_j^{(i)}$ modeled with a Laplacian distribution.

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**æ­¥éª¤1ï¼šå†™å‡ºä¼¼ç„¶å‡½æ•° / Step 1: Write Likelihood Function**

ç»™å®šè®­ç»ƒé›†ï¼Œæ•°æ®çš„è”åˆæ¦‚ç‡åˆ†å¸ƒä¸ºï¼š
Given training set, joint probability distribution of data:

$$L(\phi, \theta) = \prod_{i=1}^M P(x^{(i)}, y^{(i)} | \phi, \theta)$$

å…¶ä¸­Ï†æ˜¯ç±»åˆ«å…ˆéªŒå‚æ•°ï¼ŒÎ¸æ˜¯ç‰¹å¾åˆ†å¸ƒå‚æ•°ã€‚
where Ï† are class prior parameters, Î¸ are feature distribution parameters.

**æ­¥éª¤2ï¼šä½¿ç”¨å¯¹æ•°ä¼¼ç„¶ / Step 2: Use Log-Likelihood**

å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼š
Log-likelihood function:

$$\ell(\phi, \theta) = \sum_{i=1}^M \log P(x^{(i)}, y^{(i)} | \phi, \theta)$$

**æ­¥éª¤3ï¼šæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒæ¨¡å‹ / Step 3: Laplacian Distribution Model**

å¯¹äºå®å€¼ç‰¹å¾ $x_j$ï¼Œä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒå»ºæ¨¡ï¼š
For real-valued feature $x_j$, model with Laplacian distribution:

$$p(x_j | \mu_{jc}, \sigma_{jc}) = \frac{1}{2\sigma_{jc}} \exp\left(-\frac{|x_j - \mu_{jc}|}{\sigma_{jc}}\right)$$

å…¶ä¸­ $\mu_{jc}$ æ˜¯ç±»åˆ«cä¸‹ç‰¹å¾jçš„ä½ç½®å‚æ•°ï¼Œ$\sigma_{jc}$ æ˜¯å°ºåº¦å‚æ•°ã€‚
where $\mu_{jc}$ is location parameter for feature j in class c, $\sigma_{jc}$ is scale parameter.

**æ­¥éª¤4ï¼šæå–ç›¸å…³é¡¹ / Step 4: Extract Relevant Terms**

ä»å¯¹æ•°ä¼¼ç„¶ä¸­æå–åªä¾èµ–äº $\mu_{jc}$ å’Œ $\sigma_{jc}$ çš„é¡¹ï¼š
Extract terms from log-likelihood that depend only on $\mu_{jc}$ and $\sigma_{jc}$:

$$\ell(\mu_{jc}, \sigma_{jc}) = \sum_{i:y^{(i)}=c} \left[-\log(2\sigma_{jc}) - \frac{|x_j^{(i)} - \mu_{jc}|}{\sigma_{jc}}\right] + \text{å¸¸æ•°é¡¹}$$

**æ­¥éª¤5ï¼šä¼°è®¡ä½ç½®å‚æ•°Î¼ / Step 5: Estimate Location Parameter Î¼**

å¯¹äºæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒï¼Œä½ç½®å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ä¸­ä½æ•°ï¼š
For Laplacian distribution, MLE of location parameter is the median:

$$\mu_{jc}^* = \text{median}\{x_j^{(i)} : y^{(i)} = c\}$$

**åŸå›  / Reason:**
- æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„ä¸­ä½æ•°ç­‰äºä½ç½®å‚æ•°
- Median of Laplacian distribution equals location parameter
- å½“Î¼æ˜¯ä¸­ä½æ•°æ—¶ï¼Œå¯¹Î¼çš„å¯¼æ•°åœ¨å¤§å¤šæ•°ç‚¹ä¸ºé›¶
- When Î¼ is median, derivative w.r.t. Î¼ is zero at most points

**æ­¥éª¤6ï¼šä¼°è®¡å°ºåº¦å‚æ•°Ïƒ / Step 6: Estimate Scale Parameter Ïƒ**

å¯¹ $\sigma_{jc}$ æ±‚å¯¼å¹¶ä»¤å…¶ä¸ºé›¶ï¼š
Take derivative w.r.t. $\sigma_{jc}$ and set to zero:

$$\frac{\partial \ell}{\partial \sigma_{jc}} = -\frac{M_c}{\sigma_{jc}} + \frac{1}{\sigma_{jc}^2} \sum_{i:y^{(i)}=c} |x_j^{(i)} - \mu_{jc}| = 0$$

å…¶ä¸­ $M_c$ æ˜¯ç±»åˆ«cçš„æ ·æœ¬æ•°ã€‚
where $M_c$ is number of samples in class c.

**æ±‚è§£ / Solve:**

$$\sigma_{jc}^* = \frac{1}{M_c} \sum_{i:y^{(i)}=c} |x_j^{(i)} - \mu_{jc}^*|$$

**ç»“è®º / Conclusion:**
- ä½ç½®å‚æ•°ï¼š$\mu_{jc}^*$ = ç±»åˆ«cä¸­ç‰¹å¾jçš„ä¸­ä½æ•°
- Location parameter: $\mu_{jc}^*$ = median of feature j in class c
- å°ºåº¦å‚æ•°ï¼š$\sigma_{jc}^*$ = ç±»åˆ«cä¸­ç‰¹å¾jçš„å¹³å‡ç»å¯¹åå·®
- Scale parameter: $\sigma_{jc}^*$ = mean absolute deviation of feature j in class c

**å…³é”®è¯ / Keywords:** æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒã€ä¸­ä½æ•°ã€å¹³å‡ç»å¯¹åå·®ã€‚

---

### ä¾‹é¢˜6ï¼šLDAåéªŒæ¦‚ç‡çš„Sigmoidå½¢å¼ / Sigmoid Form of LDA Posterior

**é¢˜ç›® / Question:**  
è¯æ˜åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œçº¿æ€§åˆ¤åˆ«åˆ†æçš„åéªŒæ¦‚ç‡ $p(y=1|x; \phi, \mu, \Sigma)$ å¯ä»¥å†™æˆsigmoidå½¢å¼ï¼š
Prove that in binary classification, the posterior of linear discriminant analysis, i.e., $p(y=1|x; \phi, \mu, \Sigma)$, admits a sigmoid form:

$$p(y=1|x) = \frac{1}{1+\exp(-\theta_0 - \theta^\top x)}$$

å…¶ä¸­Î¸æ˜¯$\{\phi, \mu, \Sigma\}$çš„å‡½æ•°ã€‚æç¤ºï¼šè®°ä½ä½¿ç”¨çº¦å®š $x_0 = 1$ã€‚
where Î¸ is a function of $\{\phi, \mu, \Sigma\}$. Hint: remember to use the convention of letting $x_0 = 1$.

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**æ­¥éª¤1ï¼šä½¿ç”¨è´å¶æ–¯å®šç† / Step 1: Use Bayes' Theorem**

$$p(y=1|x) = \frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0)p(y=0)}$$

**æ­¥éª¤2ï¼šå‡è®¾é«˜æ–¯åˆ†å¸ƒ / Step 2: Assume Gaussian Distribution**

å‡è®¾æ¯ä¸ªç±»åˆ«çš„æ•°æ®æœä»é«˜æ–¯åˆ†å¸ƒï¼š
Assume data from each class follows Gaussian distribution:

$$p(x|y=c) = \mathcal{N}(x; \mu_c, \Sigma)$$

å…¶ä¸­ä¸¤ä¸ªç±»åˆ«å…±äº«ç›¸åŒçš„åæ–¹å·®çŸ©é˜µÎ£ã€‚
where both classes share the same covariance matrix Î£.

**æ­¥éª¤3ï¼šä»£å…¥é«˜æ–¯åˆ†å¸ƒ / Step 3: Substitute Gaussian Distribution**

$$p(y=1|x) = \frac{\mathcal{N}(x; \mu_1, \Sigma) \phi_1}{\mathcal{N}(x; \mu_1, \Sigma) \phi_1 + \mathcal{N}(x; \mu_0, \Sigma) \phi_0}$$

å…¶ä¸­ $\phi_1 = p(y=1)$, $\phi_0 = p(y=0)$ã€‚
where $\phi_1 = p(y=1)$, $\phi_0 = p(y=0)$.

**æ­¥éª¤4ï¼šå±•å¼€é«˜æ–¯åˆ†å¸ƒ / Step 4: Expand Gaussian Distribution**

å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š
Probability density function of multivariate Gaussian:

$$\mathcal{N}(x; \mu, \Sigma) = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu)\right)$$

**æ­¥éª¤5ï¼šä»£æ•°å˜æ¢ / Step 5: Algebraic Manipulation**

ç»è¿‡ä»£æ•°å˜æ¢ï¼ˆè¯¦ç»†æ¨å¯¼è§è¯¾ç¨‹ç¬”è®°ï¼‰ï¼Œå¾—åˆ°ï¼š
After algebraic manipulation (detailed derivation in lecture notes):

$$p(y=1|x) = \frac{1}{1 + \exp(-\theta_0 - \theta^\top x)}$$

å…¶ä¸­ï¼š
where:

$$\theta_0 = \log\frac{\phi_1}{\phi_0} - \frac{1}{2}(\mu_1^\top\Sigma^{-1}\mu_1 - \mu_0^\top\Sigma^{-1}\mu_0)$$

$$\theta = \Sigma^{-1}(\mu_1 - \mu_0)$$

**æ­¥éª¤6ï¼šä½¿ç”¨çº¦å®š $x_0 = 1$ / Step 6: Use Convention $x_0 = 1$**

å¦‚æœæˆ‘ä»¬å°† $x_0 = 1$ åŒ…å«åœ¨ç‰¹å¾å‘é‡ä¸­ï¼Œé‚£ä¹ˆï¼š
If we include $x_0 = 1$ in the feature vector, then:

$$\theta^\top x = \theta_0 x_0 + \theta_1 x_1 + \ldots + \theta_N x_N = \theta_0 + \sum_{i=1}^N \theta_i x_i$$

è¿™æ ·å¯ä»¥å°†åç½®é¡¹ $\theta_0$ æ•´åˆåˆ°æƒé‡å‘é‡ä¸­ã€‚
This allows us to incorporate the bias term $\theta_0$ into the weight vector.

**ç»“è®º / Conclusion:**
LDAçš„åéªŒæ¦‚ç‡ç¡®å®å¯ä»¥å†™æˆsigmoidå½¢å¼ï¼Œè¿™ä¸é€»è¾‘å›å½’çš„å½¢å¼ç›¸åŒï¼Œä½†LDAæœ‰æ›´å¼ºçš„åˆ†å¸ƒå‡è®¾ã€‚
LDA posterior can indeed be written in sigmoid form, same as logistic regression, but LDA has stronger distributional assumptions.

**å…³é”®è¯ / Keywords:** è´å¶æ–¯å®šç†ã€é«˜æ–¯åˆ†å¸ƒã€sigmoidå‡½æ•°ã€çº¿æ€§åˆ¤åˆ«åˆ†æã€‚

---

### ä¾‹é¢˜7ï¼šé«˜æ–¯åˆ†å¸ƒçš„å¯¹ç§°KLæ•£åº¦ / Symmetrized KL for Gaussians

**é¢˜ç›® / Question:**  
ä¸¤ä¸ªNç»´å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ $P_1 = \mathcal{N}(x; \mu_1, \Sigma_1)$ å’Œ $P_2 = \mathcal{N}(x; \mu_2, \Sigma_2)$ çš„å¯¹ç§°KLæ•£åº¦ï¼ˆJeffreysæ•£åº¦ï¼‰å®šä¹‰ä¸ºï¼š
The symmetrized KL divergence (Jeffreys divergence) between two N-dimensional multivariate Gaussian distributions $P_1 = \mathcal{N}(x; \mu_1, \Sigma_1)$ and $P_2 = \mathcal{N}(x; \mu_2, \Sigma_2)$ is defined as:

$$J(P_1, P_2) = D_{\text{KL}}(P_1\|P_2) + D_{\text{KL}}(P_2\|P_1)$$

è¯æ˜ $J(P_1, P_2)$ å¯ä»¥å†™æˆé—­å¼å½¢å¼ï¼š
Prove that $J(P_1, P_2)$ can be written in closed form as:

$$J(P_1, P_2) = \frac{1}{2}\text{tr}(\Sigma_1^{-1}\Sigma_2 + \Sigma_2^{-1}\Sigma_1 - 2I) + \frac{1}{2}(\mu_1-\mu_2)^\top(\Sigma_1^{-1}+\Sigma_2^{-1})(\mu_1-\mu_2)$$

**è¯¦ç»†è§£ç­” / Detailed Solution:**

**æ­¥éª¤1ï¼šå†™å‡ºKLæ•£åº¦çš„å®šä¹‰ / Step 1: Write Definition of KL Divergence**

$$D_{\text{KL}}(P_1\|P_2) = \mathbb{E}_{P_1}[\log P_1 - \log P_2]$$

**æ­¥éª¤2ï¼šå±•å¼€é«˜æ–¯åˆ†å¸ƒçš„å¯¹æ•° / Step 2: Expand Logarithm of Gaussian**

å¯¹äºå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼š
For multivariate Gaussian:

$$\log \mathcal{N}(x; \mu, \Sigma) = -\frac{N}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma| - \frac{1}{2}(x-\mu)^\top\Sigma^{-1}(x-\mu)$$

**æ­¥éª¤3ï¼šè®¡ç®—KLæ•£åº¦ / Step 3: Calculate KL Divergence**

$$D_{\text{KL}}(P_1\|P_2) = \mathbb{E}_{P_1}\left[\log P_1 - \log P_2\right]$$

$$= \mathbb{E}_{P_1}\left[-\frac{1}{2}\log|\Sigma_1| - \frac{1}{2}(x-\mu_1)^\top\Sigma_1^{-1}(x-\mu_1) + \frac{1}{2}\log|\Sigma_2| + \frac{1}{2}(x-\mu_2)^\top\Sigma_2^{-1}(x-\mu_2)\right]$$

**æ­¥éª¤4ï¼šä½¿ç”¨æœŸæœ›çš„æ€§è´¨ / Step 4: Use Properties of Expectation**

å¯¹äº $x \sim \mathcal{N}(\mu_1, \Sigma_1)$ï¼š
For $x \sim \mathcal{N}(\mu_1, \Sigma_1)$:

$$\mathbb{E}[(x-\mu_1)^\top\Sigma_1^{-1}(x-\mu_1)] = \text{tr}(\Sigma_1^{-1}\Sigma_1) = \text{tr}(I) = N$$

$$\mathbb{E}[(x-\mu_2)^\top\Sigma_2^{-1}(x-\mu_2)] = \text{tr}(\Sigma_2^{-1}\Sigma_1) + (\mu_1-\mu_2)^\top\Sigma_2^{-1}(\mu_1-\mu_2)$$

**æ­¥éª¤5ï¼šè®¡ç®—å¯¹ç§°KLæ•£åº¦ / Step 5: Calculate Symmetrized KL**

$$J(P_1, P_2) = D_{\text{KL}}(P_1\|P_2) + D_{\text{KL}}(P_2\|P_1)$$

ç»è¿‡è¯¦ç»†è®¡ç®—ï¼ˆä½¿ç”¨è¿¹çš„å¾ªç¯æ€§è´¨ï¼‰ï¼š
After detailed calculation (using cyclic property of trace):

$$J(P_1, P_2) = \frac{1}{2}\text{tr}(\Sigma_1^{-1}\Sigma_2 + \Sigma_2^{-1}\Sigma_1 - 2I) + \frac{1}{2}(\mu_1-\mu_2)^\top(\Sigma_1^{-1}+\Sigma_2^{-1})(\mu_1-\mu_2)$$

**å…³é”®æŠ€å·§ / Key Techniques:**
- ä½¿ç”¨è¿¹çš„å¾ªç¯æ€§è´¨ï¼š$\text{tr}(AB) = \text{tr}(BA)$
- Use cyclic property of trace: $\text{tr}(AB) = \text{tr}(BA)$
- ä½¿ç”¨äºŒæ¬¡å‹çš„æœŸæœ›å…¬å¼
- Use expectation formula for quadratic forms
- å¯¹ç§°KLæ•£åº¦æ¶ˆé™¤äº†KLæ•£åº¦çš„éå¯¹ç§°æ€§
- Symmetrized KL eliminates asymmetry of KL divergence

**å…³é”®è¯ / Keywords:** KLæ•£åº¦ã€é«˜æ–¯åˆ†å¸ƒã€è¿¹è¿ç®—ã€å¯¹ç§°æ•£åº¦ã€‚

