================================================================================
文件: Home_Assignment_2_solutions.docx
================================================================================

Home Assignment № 2 Solutions
November 3, 2025
Exercise 1
[3  points].   Poisson  distribution  is  a  discrete  probability  distribution that expresses the probability of a given number of events occurring in a ﬁxed interval of time or space if these events occur with a known constant rate and  independently  of the time  since the  last  event.   Let  x has  a  Poisson distribution,
                                          (1)
where k is an occurrence number and the parameter λ is the average number of events, and the mean and variance are the same E[x] = Var(x) = λ .
a)  [1 point].  Derive the maximum-likelihood estimate of λ, given a set of in-  dependent and identically distributed (i.i.d.) samples D = {k(1) , . . . , k(M)}.
b)  [2 points].  The following table lists the number of intervals  (maybe per minute) that are observed to have k occurrences.  The total number of intervals is 230. Please calculate the maximum likelihood estimate λ* .
Solution:
a)  The log-likelihood of the data D = {k(1) , . . . , k(M)} is
We take the derivative of L(λ) with respect to λ and set it to zero,
                                             (6)
b)  The total number of intervals is 230.  Assuming that the observation for the “4 and over” was actually 4, the maximum likelihood estimate for the Poisson distribution is
Exercise 2
[3 points].   Consider the nonlinear error surface l(u, v)  =  (uev  - 2ve-u)2 . We start at the point (u, v) = (1, 1) and minimize this error using gradient descent in the u, v space. Use Q = 0.1 (i.e., learning rate).
a)  [1 points].  What is the partial derivative of l(u, v) with respect to u?
b)  [1  point].  How many iterations does it take for the error l(u, v) to fall below 10-14 for the ﬁrst time?  In your programs, make sure to use double precision to get the needed accuracy.
c)  [1 point].   After running enough iterations such that the error has just dropped  below  10-14,  what  is  the  ﬁnal  (u, v)  you  get  in  problem  b)? Round your answer to the thousandths place.
Solution:
a)
b)  10 iterations.
c)  (0.045, 0.024).
1   #  Reference   codes   for   Exercise   2   problem  b )   and   c )
2     from   random   import   random
3     import   math 4
5     def   gr adient   decent ( fn ,   p a r t i a l   d e r i v a t i v e s  ,
6    n   variables  ,   l r = 0 . 1 , max   i ter =20 ,   toler ance =1e - 14):
7                   theta  =   [ 1 ,    1 ]
8                   y   cur  =  fn ( * theta )
9                fo r   i   in   range ( max   i ter ) :
10                               #  Calculate   gradient   of   current   theta .
11                                gr adient  =   [ f ( * theta )   fo r   f   in   p a r t i a l   d e r i v a t i v e s ]
12                               #  Update   the   theta   by   the   gradient .
13                              fo r   j   in   range ( n   v a r i a b l e s ) :
14                                              theta [ j ]  -=  gr adient [ j ]   *   l r
15                               #  Check   i f   converged   or   not .
16                                y   cur ,   y   pre  =  fn ( * theta ) ,   y   cur
17                                 i f   ( y   cur  <  t o l er a n c e ) :
18                                              print ( i +1)  #  i   st a rt s   from   0
19                                             break
20                   return   theta 21
22     de f   f ( u ,   v ) :
23                   return   ( ( u*math . exp ( v )  -  2* v*math . exp(-u ) ) * * 2 ) 24
25     def   df   du ( u ,   v ) :
26                   return   2   *   ( u*math . exp ( v )  -  2* v*math . exp(-u ) )
27                *   ( math . exp ( v )  +  2   *  v   *  math . exp(-u ) ) 28
29     def   df   d v ( u ,   v ) :
30                   return   2  *   (u*math . exp (v)  -  2* v*math . exp(-u ))
31               *   (u*math . exp (v)  -  2   *  math . exp(-u )) 32
33     def   main ( ) :
34                print ( ” Solve ·     the ·   . minimum ·     value  .   of  .   e r r o r .   s u rfa c e ·   ·   function : ” )
35                   n   v ari ables  =  2
36               para  =  gradient   decent ( f ,    [ df   du ,   df   dv ] , 37                   n   v a r i a b l e s )
38                  para  =   [ round (x ,   3)   for   x   in   para ]
39                 print ( ”The …  s o l u t i o n ·   .   i s : .   ·   (u ,  . v ) : .   .%s \n”  %   ( para ))
Exercise 3
[3 points].  Consider the following quadratic optimization problem with in-
equality constraints:
subject to
x1 + x2  ≥ 1,  x1 - 2x2  ≤ 2,
x1  ≥ 0.
This problem arises in resource allocation scenarios where we need to min- imize a quadratic cost function while satisfying multiple linear constraints.
a)  [1 point]. Write down the Lagrangian function and the KKT conditions for this optimization problem.
b)  [1 point].  Identify all points that satisfy the KKT conditions.  Which of these points is the global optimum?
c)  [1  point].   Suppose  the  ﬁrst  constraint  (x1  + x2    ≥  1)  is  relaxed  to
x1  + x2   ≥  0.5.   How would this  afect the optimal solution  and the optimal value? Explain using the KKT conditions.
Solution:
a)  [1 point] The Lagrangian function is:
L(x1 , x2 , λ 1 , λ2 , λ3 ) = x+x+2x1 +3x2 +λ1 (1-x1 -x2 )+λ2 (x1 -2x2 -2)+λ3 (-x1 )
The KKT conditions are:
λ 1 (1 - x1 - x2 ) = 0,    λ2 (x1  - 2x2 - 2) = 0,    λ3 (-x1 ) = 0,
λ 1 , λ2 , λ3  ≥ 0,
1 - x1 - x2  ≤ 0,    x1 - 2x2 - 2 ≤ 0,    -x1  ≤ 0.
b)  [1  point] To ﬁnd the  KKT points,  we analyze which constraints are active at the optimum.
• The unconstrained minimizer of the objective function is obtained from ▽f = 0, giving x1  = -2, x2  = -1.5, which is infeasible since x1   < 0 and x1  + x2   <  1.   Therefore,  some constraints must be active.
•  Consider the case where the ﬁrst constraint (x1 + x2  = 1) is active and the others are inactive. Then λ 1  > 0, λ2  = λ3  = 0.
Solving gives x1  = 1, x2  = 0, λ 1  = 3.
This point satisﬁes all constraints:
x1 + x2  = 1 ≥ 1,    x1  - 2x2  = 1 ≤ 2,    x1  = 1 ≥ 0.
The corresponding objective value is:
f (1, 0) =  (1)2 + (0)2 + 2(1) + 3(0) = 2.5.
Therefore, the unique KKT point and global optimum is:
x*  = (1, 0),    f*  = 2.5.
c) [1 point] If we relax to x1  + x2    ≥  0.5, write the new constraint as
0.5 — x1  — x2   ≤  0.  Assume only this constraint is active (veriﬁed a posteriori), then
 Hence
This point satisﬁes the remaining constraints (x1  — 2x2  = 1 ≤ 2, x1  ≥ 0). As the feasible set expands, the optimal value cannot increase; it indeed decreases from 2.5 to 13/12.
Exercise 4
[3 points]. In the formulation of SVM, we need to compute the margin (i.e., the distance) between an arbitrary point x(i)   in the N-dimensional space and a hyperplane wTx + b = 0, which can be formulated as the following optimization problem:
mxin   "x(i) — x"2     s.t.  wTx + b = 0.
a) [1 point]. Is this problem convex and why?
b) [2 points]. Using the Lagrange duality to solve for the optimal x and the distance.  (Remember to form the Lagrangian and derive the Lagrange dual function).
Solution:
a) Yes, because the objective function, L2 norm of an a伍ne function, is convex, and the equation constraint is a伍ne.
b) The given convex optimization problem is equivalent to
s.t.    wTx + b = 0.
We form the Lagrangian:
Taking the derivative of L w.r.t. x and setting it to zero, we have
                               (10)
x = x(i) — λw.
Plugging Eq.  (10) back to the Lagrangian in Eq.  (9), we arrive at the Lagrange dual function:
By maximizing g(λ), we are able to solve for λ*  :
Finally, we are able to compute the distance:
as desired.
Exercise 5
[3 points].   In the lecture note, we have given a detailed derivation of the dual form of SVM with soft margin.   With simpler arguments,  derive the
dual form of SVM with hard margin
min       wTw
s.t.    y(i)(wTx(i) + b) ≥ 1,    i = 1, . . . , M.
Compare the two dual forms.
Solution: We form the Lagrangian of the above problem
Then, we take the partial derivatives of L w.r.t. w, b and set them to zero
Plugging Eq. (11) and Eq. (12) back into the Lagrangian, we obtain
Putting  g(Q)  together  with  the  constraints,  we  obtain  the  following  dual optimization problem:
s.t.     
Qi  ≥ 0,    i = 1, . . . , M.                                          (14)
Recall that the dual problem of SVM with soft margin is
s.t.     
0 ≤ Qi  ≤ C,    i = 1, . . . , M.                                   (15)
Apparently the ranges of Qi  are diferent.
Number of occurrences (k) | 0 | 1 | 2 | 3 | 4 and over
Number of intervals with k | 100 | 81 | 34 | 9 | 6

